{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfa141a4-a900-4d62-b14e-dc21511dffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import math\n",
    "import keras_cv\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_cv.visualization as visualization\n",
    "import cv2\n",
    "import keras\n",
    "from keras_cv import models, losses, callbacks\n",
    "import keras_cv.losses as losses\n",
    "from keras_cv.models import YOLOV8Detector\n",
    "import matplotlib.patches as patches\n",
    "from tensorflow import data as tf_data\n",
    "import tensorflow_datasets as tfds\n",
    "import keras\n",
    "import keras_cv\n",
    "import numpy as np\n",
    "from keras_cv import bounding_box\n",
    "import os\n",
    "from keras_cv import visualization\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5f652e-2981-402a-bb39-daef954b47a7",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "792411ba-376d-4537-8857-2ea473b2cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (640, 640)  # Input size for YOLOv8\n",
    "BATCH_SIZE = 3  # Number of samples per batch\n",
    "NUM_CLASSES = 1  # Example number of classes, adjust as needed\n",
    "BOUNDING_BOX_FORMAT = \"xywh\"  # YOLO bounding box format\n",
    "PAD_TO_ASPECT_RATIO = True  # To maintain aspect ratio when resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69756c5b-db71-4a0d-8313-7b6ca82c4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the datasets\n",
    "TRAIN_IMAGES_DIR = Path(\"dataset/train/images/\")\n",
    "TRAIN_LABELS_DIR = Path(\"dataset/train/labels/\")\n",
    "\n",
    "VAL_IMAGES_DIR = Path(\"dataset/val/images/\")\n",
    "VAL_LABELS_DIR = Path(\"dataset/val/labels/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6680462-dfd2-4fb5-ae1d-5a65f384e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load YOLO annotations\n",
    "def load_yolo_annotations(label_path, image_size):\n",
    "    annotations = []\n",
    "    with open(label_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\" \")\n",
    "            if len(parts) != 5:\n",
    "                continue  # Skip lines that don't match expected format\n",
    "\n",
    "            class_id = int(parts[0])\n",
    "            x_center = float(parts[1])\n",
    "            y_center = float(parts[2])\n",
    "            width = float(parts[3])\n",
    "            height = float(parts[4])\n",
    "\n",
    "            # Convert normalized \"xywh\" to pixel-based \"xyxy\" format\n",
    "            x_min = (x_center - width / 2) * image_size[0]\n",
    "            y_min = (y_center - height / 2) * image_size[1]\n",
    "            x_max = (x_center + width / 2) * image_size[0]\n",
    "            y_max = (y_center + height / 2) * image_size[1]\n",
    "\n",
    "            annotations.append([x_min, y_min, x_max, y_max, class_id])\n",
    "\n",
    "    return np.array(annotations, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cb9598a-87bd-4e6e-bc15-fb359fa7d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load image and corresponding annotations\n",
    "def load_sample(image_path, labels_dir):\n",
    "    image_path_str = tf.keras.backend.get_value(image_path).decode(\"utf-8\")  # Convert tensor to string\n",
    "    image = Image.open(image_path_str).resize(IMAGE_SIZE)  # Resize to 640x640\n",
    "    image = np.array(image) / 255.0  # Normalize\n",
    "    \n",
    "    # Construct the label path and validate its existence\n",
    "    image_stem = Path(image_path_str).stem\n",
    "    label_path = os.path.join(labels_dir, image_stem + \".txt\")\n",
    "\n",
    "    if not os.path.isfile(label_path):\n",
    "        raise FileNotFoundError(f\"Label file not found: {label_path}\")\n",
    "\n",
    "    # Load YOLO annotations\n",
    "    annotations = load_yolo_annotations(label_path, IMAGE_SIZE)  # Load annotations\n",
    "    return image, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6054bcf6-104d-471c-9677-629910b78576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resize image for inference\n",
    "def inference_resizing(image, annotations):\n",
    "    print(f\"Image shape: {image.shape}, Annotations: {annotations}\")\n",
    "    resized_image = tf.image.resize(image, [640, 640])\n",
    "    return resized_image, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26b53297-b762-4c3c-b138-3c4e4fbc9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count elements in a dataset\n",
    "def count_elements(dataset):\n",
    "    return dataset.cardinality().numpy()\n",
    "\n",
    "# Check if the dataset is empty\n",
    "def is_dataset_empty(dataset):\n",
    "    return count_elements(dataset) <= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5dffbdd-4c1f-44b6-83f6-0814bdad0af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to separate class IDs from bounding box coordinates\n",
    "def extract_bounding_box_info(bounding_boxes_raw):\n",
    "    # Check if the last dimension has five elements\n",
    "    if bounding_boxes_raw.shape[-1] == 5:\n",
    "        # Extract the class ID (last element) and bounding box coordinates\n",
    "        class_ids = bounding_boxes_raw[..., -1]  # The last element is the class ID\n",
    "        bounding_boxes = bounding_boxes_raw[..., :-1]  # The rest is the bounding box coordinates\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected bounding box shape: {bounding_boxes_raw.shape}\")\n",
    "    return bounding_boxes, class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d728cb6-0d48-4513-aeeb-f5ab25b53bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize image data\n",
    "def normalize_image_data(image):\n",
    "    print(f\"Original image shape: {image.shape}\")  # Debug print\n",
    "    # Convert TensorFlow tensor to NumPy array\n",
    "    image = image.numpy()  # Explicit conversion\n",
    "    # If data is in float format, scale to [0, 255]\n",
    "    if image.dtype == np.float32 or image.dtype == np.float64:\n",
    "        image = (image * 255).astype(np.uint8)  # Scale to [0, 255]\n",
    "    print(f\"Normalized image shape: {image.shape}\")  # Debug print\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "579c499c-da86-4bfe-a6b9-e1dfd4b3d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert from BGR to RGB if needed\n",
    "def ensure_rgb_format(image):\n",
    "    # If the image appears incorrect, try converting BGR to RGB\n",
    "    if image.shape[-1] == 3:  # Assuming three channels (RGB or BGR)\n",
    "        return image[..., ::-1]  # Reverse the color channels to convert BGR to RGB\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca6ada37-47ce-4d27-8138-4027ad1baaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter out empty annotations\n",
    "def filter_empty_annotations(image, annotations):\n",
    "  return tf.size(annotations) > 0  # Check if there are any annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cafe9096-22f6-41c5-81e4-57ad239480c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_annotations(image, annotations, max_annotations=5):\n",
    "    print(f\"Image shape: {image.shape}, Annotations: {annotations}\")  # Debug print\n",
    "    num_annotations = tf.shape(annotations)[0]\n",
    "    annotations = tf.reshape(annotations, [num_annotations, 5])\n",
    "\n",
    "    padding = [[0, max_annotations - num_annotations], [0, 0]]\n",
    "    annotations = tf.pad(annotations, padding, constant_values=-1)\n",
    "\n",
    "    boxes = annotations[:, :4]\n",
    "    classes = tf.expand_dims(annotations[:, 4], axis=-1)\n",
    "    \n",
    "    return image, {'boxes': boxes, 'classes': classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c29aff6b-e9b5-4cbe-b980-908f35af6efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_loader(images_dir, labels_dir, batch_size):\n",
    "#     image_paths = list(Path(images_dir).rglob(\"*.jpg\")) + list(Path(images_dir).rglob(\"*.png\"))\n",
    "\n",
    "#     if len(image_paths) == 0:\n",
    "#         raise ValueError(f\"No images found in {images_dir}. Check your dataset path.\")\n",
    "\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices([str(p) for p in image_paths])\n",
    "\n",
    "#     def load_sample_with_shape(image_path):\n",
    "#         image, annotations = tf.py_function(\n",
    "#             lambda y: load_sample(y, labels_dir),\n",
    "#             [image_path],\n",
    "#             [tf.float32, tf.float32]\n",
    "#         )\n",
    "#         image.set_shape(IMAGE_SIZE + (3,))\n",
    "#         annotations.set_shape([None, 5])\n",
    "#         return image, annotations\n",
    "\n",
    "#     dataset = dataset.map(load_sample_with_shape, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "#     def resize_with_shape(image, annotations):\n",
    "#         image, annotations = tf.py_function(\n",
    "#             func=lambda img, ann: inference_resizing(img, ann),\n",
    "#             inp=[image, annotations],\n",
    "#             Tout=[tf.float32, tf.float32]\n",
    "#         )\n",
    "#         image.set_shape([640, 640, 3])\n",
    "#         annotations.set_shape([None, 5])\n",
    "#         return image, annotations\n",
    "\n",
    "#     dataset = dataset.map(resize_with_shape, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "#     def normalize_with_shape(image, annotations):\n",
    "#         image, annotations = tf.py_function(\n",
    "#             func=lambda img, ann: (normalize_image_data(img), ann),\n",
    "#             inp=[image, annotations],\n",
    "#             Tout=[tf.float32, tf.float32]\n",
    "#         )\n",
    "#         image.set_shape([640, 640, 3])\n",
    "#         annotations.set_shape([None, 5])\n",
    "#         return image, annotations\n",
    "\n",
    "#     dataset = dataset.map(normalize_with_shape, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "#     def ensure_rgb_with_shape(image, annotations):\n",
    "#         image, annotations = tf.py_function(\n",
    "#             func=lambda img, ann: (ensure_rgb_format(img), ann),\n",
    "#             inp=[image, annotations],\n",
    "#             Tout=[tf.float32, tf.float32]\n",
    "#         )\n",
    "#         image.set_shape([640, 640, 3])\n",
    "#         annotations.set_shape([None, 5])\n",
    "#         return image, annotations\n",
    "\n",
    "#     dataset = dataset.map(ensure_rgb_with_shape, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "#     dataset = dataset.filter(lambda image, annotations: tf.py_function(\n",
    "#         func=filter_empty_annotations,\n",
    "#         inp=[image, annotations],\n",
    "#         Tout=tf.bool)\n",
    "#     )\n",
    "\n",
    "#     dataset = dataset.map(lambda image, annotations: pad_annotations(image, annotations))\n",
    "\n",
    "#     dataset = dataset.batch(batch_size, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#     return dataset\n",
    "\n",
    "\n",
    "# Data loader with flexible batch size and error handling\n",
    "def data_loader(images_dir, labels_dir, batch_size):\n",
    "  image_paths = list(Path(images_dir).rglob(\"*.jpg\")) + list(Path(images_dir).rglob(\"*.png\"))\n",
    "\n",
    "  if len(image_paths) == 0:\n",
    "    raise ValueError(f\"No images found in {images_dir}. Check your dataset path.\")\n",
    "\n",
    "  # Create TensorFlow dataset object from list of image paths\n",
    "  dataset = tf.data.Dataset.from_tensor_slices([str(p) for p in image_paths])\n",
    "\n",
    "  # Map function to load images and annotations with error handling\n",
    "  dataset = dataset.map(\n",
    "    lambda x: tf.py_function(\n",
    "      lambda y: load_sample(y, labels_dir),\n",
    "      [x],\n",
    "      [tf.float32, tf.float32]\n",
    "    ),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    "  )\n",
    "\n",
    "  # Apply the resizing function (optional, comment out if not needed)\n",
    "  # dataset = dataset.map(\n",
    "  #   lambda image, annotations: tf.py_function(\n",
    "  #     func=lambda img, ann: inference_resizing(img, ann),\n",
    "  #     inp=[image, annotations],\n",
    "  #     Tout=[tf.float32, tf.float32]\n",
    "  #   ),\n",
    "  #   num_parallel_calls=tf.data.AUTOTUNE,\n",
    "  # )\n",
    "\n",
    "  # Normalize image data\n",
    "  dataset = dataset.map(\n",
    "    lambda image, annotations: tf.py_function(\n",
    "      func=lambda img, ann: (normalize_image_data(img), ann),\n",
    "      inp=[image, annotations],\n",
    "      Tout=[tf.float32, tf.float32]\n",
    "    ),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    "  )\n",
    "\n",
    "  # Ensure RGB format\n",
    "  dataset = dataset.map(\n",
    "    lambda image, annotations: tf.py_function(\n",
    "      func=lambda img, ann: (ensure_rgb_format(img), ann),\n",
    "      inp=[image, annotations],\n",
    "      Tout=[tf.float32, tf.float32]\n",
    "    ),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    "  )\n",
    "\n",
    "  # Filter empty annotations\n",
    "  dataset = dataset.filter(filter_empty_annotations)\n",
    "\n",
    "  # Pad annotations (optional, uncomment if needed)\n",
    "  # dataset = dataset.map(lambda image, annotations: pad_annotations(image, annotations))\n",
    "\n",
    "  # Apply batching, allowing for partial batches\n",
    "  dataset = dataset.batch(batch_size, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "84af5ea1-f503-408c-8ee4-b89a7be5af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for training, validation, and testing\n",
    "train_dataset = data_loader(TRAIN_IMAGES_DIR, TRAIN_LABELS_DIR, BATCH_SIZE)\n",
    "val_dataset = data_loader(VAL_IMAGES_DIR, VAL_LABELS_DIR, BATCH_SIZE // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a7b722-915d-49b3-b343-6fbf2c0c162b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
