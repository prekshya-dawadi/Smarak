{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd1be4f1-92ee-4f92-a8d3-cec66fe2d43e",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd1374f1-5dad-4cc5-94e5-a5d1c0244b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "import logging  # To log errors or missing files\n",
    "from collections import namedtuple\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f99a5446-1b21-431e-b40b-b588e13ebb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configures basic logging to print informational messages\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56dec583-c3f0-45a9-967e-f3ceab3489f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = \"annotations.json\"\n",
    "image_folder = \"coco-nyaptola/images\"\n",
    "\n",
    "# Convert to absolute paths for reliable file access\n",
    "annotation_file = os.path.abspath(annotation_file)\n",
    "image_folder = os.path.abspath(image_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca214c27-88e7-4b57-85b6-518ed0795e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageData = namedtuple('ImageData',['image','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27fbd3e1-cfa0-42c9-9bea-dcdf5aab83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3624599e-7ea9-42e6-b8bf-020fbc65399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations from a COCO JSON file\n",
    "def load_annotations(annotation_file):\n",
    "    with open(annotation_file, 'r') as f:\n",
    "        annotations = json.load(f) #loaded annotations dictionary\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bbcce3c-8471-442a-be8d-b99658c2cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_and_annotations(image_path, annotations):\n",
    "    # Initialize an empty image path string\n",
    "    image_path_str = \"\"\n",
    "\n",
    "    try:\n",
    "        # Ensure that the input is a TensorFlow tensor\n",
    "        if isinstance(image_path, tf.Tensor):\n",
    "            # Convert Tensor to Python string for file operations\n",
    "            image_path_str = image_path.numpy().decode(\"utf-8\")\n",
    "        else:\n",
    "            raise ValueError(\"Expected TensorFlow tensor for image path.\")\n",
    "\n",
    "        # Read the image from the file path\n",
    "        image = tf.io.decode_png(tf.io.read_file(image_path_str), channels=3)\n",
    "\n",
    "        # Get the image information from the annotations\n",
    "        image_info = next(\n",
    "            (img for img in annotations['images'] if os.path.basename(image_path_str) in img.values()),\n",
    "            None\n",
    "        )\n",
    "\n",
    "        if not image_info:\n",
    "            raise ValueError(f\"Image '{image_path_str}' not found in annotations.\")\n",
    "\n",
    "        image_id = image_info['id']\n",
    "\n",
    "        # Get bounding boxes and classes for the image\n",
    "        bboxes = [\n",
    "            ann['bbox'] for ann in annotations['annotations']\n",
    "            if ann['image_id'] == image_id\n",
    "        ]\n",
    "        classes = [\n",
    "            ann['category_id'] for ann in annotations['annotations']\n",
    "            if ann['image_id'] == image_id\n",
    "        ]\n",
    "\n",
    "        return image, {'boxes': bboxes, 'classes': classes}\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the error and return default values\n",
    "        logging.warning(f\"Error loading image: {e}\")\n",
    "        return tf.zeros([1, 1, 3], dtype=tf.uint8), {'boxes': [], 'classes': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06e1ad4e-c3f8-4d6f-941d-179599ce5fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(annotation_file, image_folder):\n",
    "\n",
    "    # Create a tf.data.Dataset with images, bounding boxes, and classes from a COCO-format dataset.\n",
    "    annotations = load_annotations(annotation_file)\n",
    "\n",
    "    # Create a list of image file paths\n",
    "    image_files = [\n",
    "        tf.convert_to_tensor(os.path.join(image_folder, str(img['image_id'])), dtype=tf.string)\n",
    "        for img in annotations\n",
    "    ]\n",
    "\n",
    "    # Convert image file paths to TensorFlow-compatible tensors\n",
    "    image_files = [tf.convert_to_tensor(img, dtype=tf.string) for img in image_files]\n",
    "\n",
    "    # Correct the bounding box data in load_data\n",
    "    def load_data(image_file):\n",
    "        image, target = load_image_and_annotations(image_file, annotations)\n",
    "    \n",
    "        # Ensure that bounding boxes have the correct shape\n",
    "        bboxes = target['boxes']\n",
    "    \n",
    "        # Correct for empty or incorrect shapes\n",
    "        if len(bboxes) == 0:\n",
    "            bboxes = tf.zeros((0, 4), dtype=tf.float32)  # Correct shape for empty bounding boxes\n",
    "        else:\n",
    "            bboxes = tf.convert_to_tensor(bboxes, dtype=tf.float32)\n",
    "\n",
    "        classes = tf.convert_to_tensor(target['classes'], dtype=tf.int32)\n",
    "\n",
    "        return image, {'boxes': bboxes, 'classes': classes}\n",
    "\n",
    "\n",
    "    # Map function with error handling\n",
    "\n",
    "    # def map_function(image_file):\n",
    "    #     image, bboxes, classes = tf.py_function(\n",
    "    #         load_data,\n",
    "    #         [image_file],\n",
    "    #         Tout=(tf.uint8, tf.float32, tf.int32),\n",
    "    #     )\n",
    "\n",
    "    #     return image, bboxes, classes\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(image_files).map(\n",
    "        load_data)\n",
    "        \n",
    "    def filter_data(image, bounding_boxes, dataset):\n",
    "      return image is not None\n",
    "\n",
    "    dataset = dataset.filter(lambda image, bounding_boxes: filter_data(image, bounding_boxes, dataset))\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a91fa9c4-4f4a-443c-bcc9-e356076d3943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Error loading image: 'SymbolicTensor' object has no attribute 'numpy'\n"
     ]
    }
   ],
   "source": [
    "train_dataset = data_loader(annotation_file, image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4aa15438-f878-4f22-8037-bda36d285231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.filter_op._FilterDataset'>\n",
      "(TensorSpec(shape=(1, 1, 3), dtype=tf.uint8, name=None), {'boxes': TensorSpec(shape=(0, 4), dtype=tf.float32, name=None), 'classes': TensorSpec(shape=(0,), dtype=tf.int32, name=None)})\n"
     ]
    }
   ],
   "source": [
    "# prints the type of train_dataset. \n",
    "print(type(train_dataset))  # Should be <class 'tensorflow.python.data.ops.dataset_ops.DatasetV2'>\n",
    "print(train_dataset.element_spec)  # Should reflect the expected structure of elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49f7f093-42bb-4972-b6cf-c060a12e6af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (1, 1, 3)\n",
      "target: {'boxes': <tf.Tensor: shape=(0, 4), dtype=float32, numpy=array([], shape=(0, 4), dtype=float32)>, 'classes': <tf.Tensor: shape=(0,), dtype=int32, numpy=array([], dtype=int32)>}\n",
      "Image shape: (1, 1, 3)\n",
      "target: {'boxes': <tf.Tensor: shape=(0, 4), dtype=float32, numpy=array([], shape=(0, 4), dtype=float32)>, 'classes': <tf.Tensor: shape=(0,), dtype=int32, numpy=array([], dtype=int32)>}\n",
      "Image shape: (1, 1, 3)\n",
      "target: {'boxes': <tf.Tensor: shape=(0, 4), dtype=float32, numpy=array([], shape=(0, 4), dtype=float32)>, 'classes': <tf.Tensor: shape=(0,), dtype=int32, numpy=array([], dtype=int32)>}\n",
      "Image shape: (1, 1, 3)\n",
      "target: {'boxes': <tf.Tensor: shape=(0, 4), dtype=float32, numpy=array([], shape=(0, 4), dtype=float32)>, 'classes': <tf.Tensor: shape=(0,), dtype=int32, numpy=array([], dtype=int32)>}\n",
      "Image shape: (1, 1, 3)\n",
      "target: {'boxes': <tf.Tensor: shape=(0, 4), dtype=float32, numpy=array([], shape=(0, 4), dtype=float32)>, 'classes': <tf.Tensor: shape=(0,), dtype=int32, numpy=array([], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "for data in train_dataset.take(5):  # Adjust the number of samples\n",
    "    image, target = data\n",
    "    print(\"Image shape:\", image.shape)  # Check the image shape and other attributes\n",
    "    print(\"target:\", target)  # Check bounding boxes and classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af83506b-c15e-46f4-aeab-29fb710c759a",
   "metadata": {},
   "source": [
    "## Data Visualization and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3ae32dbc-6022-4bbb-b31c-763b5646a706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import keras\n",
    "import keras_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ad12d8cd-22a3-4819-b60b-f44fe28dd97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.filter_op._FilterDataset"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d4ae675b-10c8-4b1f-ae28-21f1223f19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of json files, hardcoded for now\n",
    "length = 100\n",
    "SPLIT_RATIO = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5cc70f8f-0e19-4d0d-bf4f-266ad0f5b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of validation samples\n",
    "num_val = int(length * SPLIT_RATIO)\n",
    " \n",
    "# Split the dataset into train and validation sets\n",
    "val_data = train_dataset.take(num_val)\n",
    "train_data = train_dataset.skip(num_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "30fd4920-e6e9-4c8f-8cb9-30d042065e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format, class_mapping):\n",
    "    # Extract a single batch from the dataset\n",
    "    inputs = next(iter(inputs.take(1)))\n",
    "\n",
    "    image = inputs[0]\n",
    "    bounding_box_info = inputs[1]\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    bounding_boxes = bounding_box_info[\"boxes\"].numpy()\n",
    "    classes = bounding_box_info[\"classes\"].numpy()\n",
    "\n",
    "    # Ensure the image is a batch of images\n",
    "    if len(image.shape) == 3:  # If it's a single image\n",
    "        image = tf.expand_dims(image, axis=0)  # Convert to a batch of 1\n",
    "\n",
    "    bounding_box_data = {\"boxes\": bounding_boxes, \"classes\": classes}\n",
    "\n",
    "    # Visualization function call with the corrected shape\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        image.numpy(),  # Convert image to NumPy with proper shape\n",
    "        value_range=value_range,\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        y_true=bounding_box_data,  # Pass bounding box data\n",
    "        scale=5,\n",
    "        font_scale=0.7,\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        class_mapping=class_mapping,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "69bb1a69-c102-4d5e-ab14-f7bb70438a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = { 0: \"nyatapola\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "09d43f83-655e-4254-9b54-f20aa12c28bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvisualize_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounding_box_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxyxy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_mapping\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclass_mapping\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[64], line 19\u001b[0m, in \u001b[0;36mvisualize_dataset\u001b[1;34m(inputs, value_range, rows, cols, bounding_box_format, class_mapping)\u001b[0m\n\u001b[0;32m     16\u001b[0m bounding_box_data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m: bounding_boxes, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m\"\u001b[39m: classes}\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Visualization function call with the corrected shape\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[43mvisualization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_bounding_box_gallery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Convert image to NumPy with proper shape\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounding_box_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass bounding box data\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfont_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbounding_box_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounding_box_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\object_detection\\Lib\\site-packages\\keras_cv\\src\\visualization\\plot_bounding_box_gallery.py:142\u001b[0m, in \u001b[0;36mplot_bounding_box_gallery\u001b[1;34m(images, value_range, bounding_box_format, y_true, y_pred, true_color, pred_color, line_thickness, font_scale, text_thickness, class_mapping, ground_truth_mapping, prediction_mapping, legend, legend_handles, rows, cols, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m     y_true[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mto_numpy(y_true[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    139\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m bounding_box\u001b[38;5;241m.\u001b[39mconvert_format(\n\u001b[0;32m    140\u001b[0m         y_true, images\u001b[38;5;241m=\u001b[39mimages, source\u001b[38;5;241m=\u001b[39mbounding_box_format, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxyxy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m     )\n\u001b[1;32m--> 142\u001b[0m     plotted_images \u001b[38;5;241m=\u001b[39m \u001b[43mdraw_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mplotted_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mground_truth_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_pred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    150\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\object_detection\\Lib\\site-packages\\keras_cv\\src\\visualization\\draw_bounding_boxes.py:88\u001b[0m, in \u001b[0;36mdraw_bounding_boxes\u001b[1;34m(images, bounding_boxes, color, bounding_box_format, line_thickness, text_thickness, font_scale, class_mapping)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImages must be a batched np-like with elements of shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(height, width, 3)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     84\u001b[0m     )\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m     87\u001b[0m     bounding_box_batch \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mbounding_boxes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboxes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m\"\u001b[39m: bounding_boxes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m\"\u001b[39m][i],\n\u001b[0;32m     90\u001b[0m     }\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m bounding_boxes:\n\u001b[0;32m     92\u001b[0m         bounding_box_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m bounding_boxes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "visualize_dataset(\n",
    "    train_data, bounding_box_format=\"xyxy\", value_range=(0, 255), rows=2, cols=2, class_mapping = class_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55170df1-2eb6-4fbe-962a-c9f0a4e9a22b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
