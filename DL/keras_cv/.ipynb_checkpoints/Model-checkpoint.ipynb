{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa141a4-a900-4d62-b14e-dc21511dffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import math\n",
    "import keras_cv\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_cv.visualization as visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5f652e-2981-402a-bb39-daef954b47a7",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "792411ba-376d-4537-8857-2ea473b2cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (640, 640)  # Input size for YOLOv8\n",
    "BATCH_SIZE = 16  # Number of samples per batch\n",
    "NUM_CLASSES = 1  # Example number of classes, adjust as needed\n",
    "BOUNDING_BOX_FORMAT = \"xywh\"  # YOLO bounding box format\n",
    "PAD_TO_ASPECT_RATIO = True  # To maintain aspect ratio when resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69756c5b-db71-4a0d-8313-7b6ca82c4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the datasets\n",
    "TRAIN_IMAGES_DIR = \"dataset/train/images/\"\n",
    "TRAIN_LABELS_DIR = \"dataset/train/labels/\"\n",
    "\n",
    "VAL_IMAGES_DIR = \"dataset/val/images/\"\n",
    "VAL_LABELS_DIR = \"dataset/val/labels/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e719a1b1-b7a4-4697-9f5b-2dfa4b89ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess images\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).resize(IMAGE_SIZE)  # Resize\n",
    "    image = np.array(image) / 255.0  # Normalize\n",
    "    return image.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e6680462-dfd2-4fb5-ae1d-5a65f384e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load YOLO annotations\n",
    "def load_yolo_annotations(label_path, image_size):\n",
    "    annotations = []\n",
    "    with open(label_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\" \")\n",
    "            if len(parts) != 5:\n",
    "                continue  # Skip lines that don't match expected format\n",
    "\n",
    "            class_id = int(parts[0])\n",
    "            x_center = float(parts[1])\n",
    "            y_center = float(parts[2])\n",
    "            width = float(parts[3])\n",
    "            height = float(parts[4])\n",
    "\n",
    "            # Convert normalized \"xywh\" to pixel-based \"xyxy\" format\n",
    "            x_min = (x_center - width / 2) * image_size[0]\n",
    "            y_min = (y_center - height / 2) * image_size[1]\n",
    "            x_max = (x_center + width / 2) * image_size[0]\n",
    "            y_max = (y_center + height / 2) * image_size[1]\n",
    "\n",
    "            annotations.append([x_min, y_min, x_max, y_max, class_id])\n",
    "\n",
    "    return np.array(annotations, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3cb9598a-87bd-4e6e-bc15-fb359fa7d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load image and corresponding annotations\n",
    "def load_sample(image_path, labels_dir):\n",
    "    image_path_str = tf.keras.backend.get_value(image_path).decode(\"utf-8\")  # Convert tensor to string\n",
    "    image = Image.open(image_path_str).resize(IMAGE_SIZE)  # Resize to 640x640\n",
    "    image = np.array(image) / 255.0  # Normalize\n",
    "    \n",
    "    # Construct the label path and validate its existence\n",
    "    image_stem = Path(image_path_str).stem\n",
    "    label_path = os.path.join(labels_dir, image_stem + \".txt\")\n",
    "\n",
    "    if not os.path.isfile(label_path):\n",
    "        raise FileNotFoundError(f\"Label file not found: {label_path}\")\n",
    "\n",
    "    # Load YOLO annotations\n",
    "    annotations = load_yolo_annotations(label_path, IMAGE_SIZE)  # Load annotations\n",
    "    return image, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c29aff6b-e9b5-4cbe-b980-908f35af6efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader with flexible batch size and error handling\n",
    "def data_loader(images_dir, labels_dir, batch_size):\n",
    "    image_paths = list(Path(images_dir).rglob(\"*.jpg\")) + list(Path(images_dir).rglob(\"*.png\"))\n",
    "    \n",
    "    if len(image_paths) == 0:\n",
    "        raise ValueError(f\"No images found in {images_dir}. Check your dataset path.\")\n",
    "\n",
    "    # Create TensorFlow dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices([str(p) for p in image_paths])\n",
    "\n",
    "    # Map function to load images and annotations with error handling\n",
    "    dataset = dataset.map(\n",
    "        lambda x: tf.py_function(\n",
    "            lambda y: load_sample(y, labels_dir),\n",
    "            [x],\n",
    "            [tf.float32, tf.float32]\n",
    "        ),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    )\n",
    "\n",
    "    # Apply batching, allowing for partial batches\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6054bcf6-104d-471c-9677-629910b78576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a resizing layer for inference\n",
    "inference_resizing = keras_cv.layers.Resizing(\n",
    "    IMAGE_SIZE[0], IMAGE_SIZE[1],\n",
    "    bounding_box_format=BOUNDING_BOX_FORMAT,\n",
    "    pad_to_aspect_ratio=PAD_TO_ASPECT_RATIO\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26b53297-b762-4c3c-b138-3c4e4fbc9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count elements in a dataset\n",
    "def count_elements(dataset):\n",
    "    return dataset.cardinality().numpy()\n",
    "\n",
    "# Check if the dataset is empty\n",
    "def is_dataset_empty(dataset):\n",
    "    return count_elements(dataset) <= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2028e889-15af-4674-9509-628dac435060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary visualization libraries\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5dffbdd-4c1f-44b6-83f6-0814bdad0af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to separate class IDs from bounding box coordinates\n",
    "def extract_bounding_box_info(bounding_boxes_raw):\n",
    "    # Check if the last dimension has five elements\n",
    "    if bounding_boxes_raw.shape[-1] == 5:\n",
    "        # Extract the class ID (last element) and bounding box coordinates\n",
    "        class_ids = bounding_boxes_raw[..., -1]  # The last element is the class ID\n",
    "        bounding_boxes = bounding_boxes_raw[..., :-1]  # The rest is the bounding box coordinates\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected bounding box shape: {bounding_boxes_raw.shape}\")\n",
    "    return bounding_boxes, class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d728cb6-0d48-4513-aeeb-f5ab25b53bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize image data\n",
    "def normalize_image_data(image):\n",
    "    # Convert TensorFlow tensor to NumPy array\n",
    "    image = image.numpy()  # Explicit conversion\n",
    "    # If data is in float format, scale to [0, 255]\n",
    "    if image.dtype == np.float32 or image.dtype == np.float64:\n",
    "        image = (image * 255).astype(np.uint8)  # Scale to [0, 255]\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "579c499c-da86-4bfe-a6b9-e1dfd4b3d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert from BGR to RGB if needed\n",
    "def ensure_rgb_format(image):\n",
    "    # If the image appears incorrect, try converting BGR to RGB\n",
    "    if image.shape[-1] == 3:  # Assuming three channels (RGB or BGR)\n",
    "        return image[..., ::-1]  # Reverse the color channels to convert BGR to RGB\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b7b72f97-5958-4bac-9887-a37bd090f3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Visualization function with error handling for large images\n",
    "def visualize_dataset(dataset, max_rows=2, max_cols=2, image_size=(224, 224), max_images=4):\n",
    "    # Visualize in smaller batches to avoid memory overload\n",
    "    batch = next(iter(dataset.take(1)))  # Get the first batch\n",
    "    images, bounding_boxes_raw = batch\n",
    "\n",
    "\n",
    "    # Extract images and raw bounding boxes\n",
    "    images, bounding_boxes_raw = batch\n",
    "    \n",
    "    # Normalize images for plotting\n",
    "    images = [image.numpy() for image in images]\n",
    "\n",
    "    # Cap the number of images to visualize\n",
    "    total_images = min(len(images), max_images)\n",
    "\n",
    "    # Determine the number of subplots required\n",
    "    rows = min(total_images, max_rows)  # Ensure rows do not exceed total images\n",
    "    cols = min(math.ceil(total_images / rows), max_cols)  # Ensure valid subplot count\n",
    "    \n",
    "    # Adjust the figure size\n",
    "    plt.figure(figsize=(4, 4), dpi=80)  # Smaller figure size and lower DPI\n",
    "\n",
    "    for i in range(total_images):\n",
    "        plt.subplot(rows, cols, i + 1)  # Define subplot layout\n",
    "        plt.imshow(images[i])  # Display the image\n",
    "        plt.axis(\"off\")  # Hide axis for better visualization\n",
    "\n",
    "        # Extract bounding boxes and class IDs\n",
    "        bounding_boxes = bounding_boxes_raw[i].numpy()\n",
    "\n",
    "        # Plot the bounding boxes\n",
    "        for box in bounding_boxes:\n",
    "            class_id = int(box[0])  # Class ID\n",
    "            x_center = box[1] * image_size[0]  # Convert from normalized to pixel\n",
    "            y_center = box[2] * image_size[1]\n",
    "            width = box[3] * image_size[0]\n",
    "            height = box[4] * image_size[1]\n",
    "\n",
    "            # Convert \"xywh\" to \"xyxy\"\n",
    "            x_min = x_center - width / 2\n",
    "            y_min = y_center - height / 2\n",
    "            x_max = x_center + width / 2\n",
    "            y_max = y_center + height / 2\n",
    "\n",
    "            # Plot bounding boxes\n",
    "            plt.gca().add_patch(\n",
    "                plt.Rectangle(\n",
    "                    (x_min, y_min),  # Top-left corner\n",
    "                    x_max - x_min,  # Width\n",
    "                    y_max - y_min,  # Height\n",
    "                    edgecolor='red',  # Outline color\n",
    "                    facecolor='none',  # No fill\n",
    "                    linewidth=2,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Annotate class ID\n",
    "            plt.text(\n",
    "                x_min, y_min - 10,\n",
    "                f\"Class {class_id}\",\n",
    "                color='red',  # Text color\n",
    "                fontsize=10,\n",
    "                backgroundcolor='white'  # Background for visibility\n",
    "            )\n",
    "\n",
    "    # Avoid tight_layout issues by reducing excess padding/margins\n",
    "    try:\n",
    "        plt.tight_layout()  # Adjust layout to avoid overlap\n",
    "    except UserWarning:\n",
    "        pass  # If tight layout doesn't work, proceed without it\n",
    "\n",
    "    plt.show()  # Display the plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "84af5ea1-f503-408c-8ee4-b89a7be5af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for training, validation, and testing\n",
    "train_dataset = data_loader(TRAIN_IMAGES_DIR, TRAIN_LABELS_DIR, BATCH_SIZE)\n",
    "val_dataset = data_loader(VAL_IMAGES_DIR, VAL_LABELS_DIR, BATCH_SIZE//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ec0a006c-f8e3-46e0-8060-81f67b7b0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6f7dc7ef-82e3-4a7b-9928-9bf57e9ed6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ca026e20-ac19-4c44-9f83-6b4d0b5c3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_resizing = keras_cv.layers.Resizing(\n",
    "    640, 640, bounding_box_format=\"xywh\", pad_to_aspect_ratio=True\n",
    ")\n",
    "val_dataset = val_dataset.map(inference_resizing, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d42cef-c121-4f6c-9c6f-4019e5186768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
