{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38121443-a5d6-4c86-a54b-d8ddff6fa289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Projects\\\\DL\\\\MonumentDetection\\\\DL'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "cfa141a4-a900-4d62-b14e-dc21511dffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import math\n",
    "import keras_cv\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_cv.visualization as visualization\n",
    "import cv2\n",
    "import keras\n",
    "from keras_cv import models, losses, callbacks\n",
    "import keras_cv.losses as losses\n",
    "from keras_cv.models import YOLOV8Detector\n",
    "import matplotlib.patches as patches\n",
    "from tensorflow import data as tf_data\n",
    "import tensorflow_datasets as tfds\n",
    "import keras\n",
    "import keras_cv\n",
    "import numpy as np\n",
    "from keras_cv import bounding_box\n",
    "import os\n",
    "from keras_cv import visualization\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5f652e-2981-402a-bb39-daef954b47a7",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "792411ba-376d-4537-8857-2ea473b2cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (640, 640)  # Input size for YOLOv8\n",
    "BATCH_SIZE = 3  # Number of samples per batch\n",
    "NUM_CLASSES = 1  # Example number of classes, adjust as needed\n",
    "BOUNDING_BOX_FORMAT = \"xywh\"  # YOLO bounding box format\n",
    "PAD_TO_ASPECT_RATIO = True  # To maintain aspect ratio when resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "69756c5b-db71-4a0d-8313-7b6ca82c4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the datasets\n",
    "TRAIN_IMAGES_DIR = Path(\"dataset/train/images/\")\n",
    "TRAIN_LABELS_DIR = Path(\"dataset/train/labels/\")\n",
    "\n",
    "VAL_IMAGES_DIR = Path(\"dataset/val/images/\")\n",
    "VAL_LABELS_DIR = Path(\"dataset/val/labels/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ee882b3d-88c7-4758-a8a9-f79cff9fea6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 612, 3)\n",
      "(338, 509, 3)\n",
      "(960, 545, 3)\n",
      "(612, 408, 3)\n",
      "(408, 612, 3)\n",
      "(413, 612, 3)\n",
      "(612, 408, 3)\n",
      "(612, 408, 3)\n",
      "(406, 612, 3)\n",
      "(612, 343, 3)\n",
      "(360, 640, 3)\n",
      "(612, 408, 3)\n",
      "(407, 612, 3)\n",
      "(667, 1000, 3)\n",
      "(612, 408, 3)\n",
      "(408, 612, 3)\n",
      "(612, 476, 3)\n",
      "(612, 408, 3)\n",
      "(612, 392, 3)\n",
      "(408, 612, 3)\n",
      "(612, 408, 3)\n",
      "(400, 612, 3)\n",
      "(360, 540, 3)\n",
      "(330, 660, 3)\n",
      "(612, 416, 3)\n",
      "(408, 612, 3)\n",
      "(612, 392, 3)\n",
      "(408, 612, 3)\n",
      "(338, 507, 3)\n",
      "(612, 398, 3)\n",
      "(612, 408, 3)\n",
      "(612, 408, 3)\n",
      "(612, 408, 3)\n",
      "(408, 612, 3)\n",
      "(612, 408, 3)\n",
      "(612, 408, 3)\n",
      "(408, 612, 3)\n",
      "(612, 408, 3)\n",
      "(408, 612, 3)\n",
      "(720, 960, 3)\n",
      "(339, 509, 3)\n",
      "(612, 413, 3)\n",
      "(360, 640, 3)\n",
      "(408, 612, 3)\n",
      "(360, 458, 3)\n",
      "(408, 612, 3)\n",
      "(490, 612, 3)\n",
      "(612, 406, 3)\n",
      "(612, 408, 3)\n",
      "(459, 612, 3)\n",
      "(338, 509, 3)\n",
      "(408, 612, 3)\n",
      "(408, 612, 3)\n",
      "(640, 516, 3)\n",
      "(400, 612, 3)\n",
      "(339, 509, 3)\n",
      "(408, 612, 3)\n",
      "(428, 612, 3)\n",
      "(612, 408, 3)\n",
      "(612, 408, 3)\n",
      "(612, 392, 3)\n",
      "(612, 591, 3)\n",
      "(408, 612, 3)\n",
      "(612, 408, 3)\n",
      "(612, 408, 3)\n",
      "(408, 612, 3)\n",
      "(438, 612, 3)\n",
      "(667, 1000, 3)\n",
      "(612, 408, 3)\n",
      "(408, 612, 3)\n",
      "(612, 406, 3)\n",
      "(360, 540, 3)\n",
      "(900, 600, 3)\n",
      "(360, 539, 3)\n",
      "(408, 612, 3)\n",
      "(344, 612, 3)\n",
      "(612, 464, 3)\n",
      "(612, 408, 3)\n",
      "(459, 612, 3)\n",
      "(408, 612, 3)\n"
     ]
    }
   ],
   "source": [
    "# Assuming TRAIN_IMAGES_DIR is a directory containing image file paths\n",
    "for filename in os.listdir(TRAIN_IMAGES_DIR):\n",
    "    image_path = TRAIN_IMAGES_DIR / filename  # Construct the full image path\n",
    "    if not image_path.exists():\n",
    "        print(f\"File does not exist: {image_path}\")\n",
    "        continue\n",
    "\n",
    "    image = cv2.imread(str(image_path))  # Convert to string if necessary for cv2.imread\n",
    "    \n",
    "    if image is None:\n",
    "        print(f\"Failed to read image: {image_path}\")\n",
    "    else:\n",
    "        print(image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e6680462-dfd2-4fb5-ae1d-5a65f384e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load YOLO annotations\n",
    "def load_yolo_annotations(label_path, image_size):\n",
    "    annotations = []\n",
    "    with open(label_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\" \")\n",
    "            if len(parts) != 5:\n",
    "                continue  # Skip lines that don't match expected format\n",
    "\n",
    "            class_id = int(parts[0])\n",
    "            x_center = float(parts[1])\n",
    "            y_center = float(parts[2])\n",
    "            width = float(parts[3])\n",
    "            height = float(parts[4])\n",
    "\n",
    "            # Convert normalized \"xywh\" to pixel-based \"xyxy\" format\n",
    "            x_min = (x_center - width / 2) * image_size[0]\n",
    "            y_min = (y_center - height / 2) * image_size[1]\n",
    "            x_max = (x_center + width / 2) * image_size[0]\n",
    "            y_max = (y_center + height / 2) * image_size[1]\n",
    "\n",
    "            annotations.append([x_min, y_min, x_max, y_max, class_id])\n",
    "\n",
    "    return np.array(annotations, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3cb9598a-87bd-4e6e-bc15-fb359fa7d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load image and corresponding annotations\n",
    "def load_sample(image_path, labels_dir):\n",
    "    image_path_str = tf.keras.backend.get_value(image_path).decode(\"utf-8\")  # Convert tensor to string\n",
    "    image = Image.open(image_path_str).resize(IMAGE_SIZE)  # Resize to 640x640\n",
    "    image = np.array(image) / 255.0  # Normalize\n",
    "    \n",
    "    # Construct the label path and validate its existence\n",
    "    image_stem = Path(image_path_str).stem\n",
    "    label_path = os.path.join(labels_dir, image_stem + \".txt\")\n",
    "\n",
    "    if not os.path.isfile(label_path):\n",
    "        raise FileNotFoundError(f\"Label file not found: {label_path}\")\n",
    "\n",
    "    # Load YOLO annotations\n",
    "    annotations = load_yolo_annotations(label_path, IMAGE_SIZE)  # Load annotations\n",
    "    return image, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "6054bcf6-104d-471c-9677-629910b78576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resize image for inference\n",
    "def inference_resizing(image, annotations):\n",
    "    print(f\"Image shape: {image.shape}, Annotations: {annotations}\")\n",
    "    resized_image = tf.image.resize(image, [224, 224])\n",
    "    return resized_image, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "26b53297-b762-4c3c-b138-3c4e4fbc9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count elements in a dataset\n",
    "def count_elements(dataset):\n",
    "    return dataset.cardinality().numpy()\n",
    "\n",
    "# Check if the dataset is empty\n",
    "def is_dataset_empty(dataset):\n",
    "    return count_elements(dataset) <= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "a5dffbdd-4c1f-44b6-83f6-0814bdad0af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to separate class IDs from bounding box coordinates\n",
    "def extract_bounding_box_info(bounding_boxes_raw):\n",
    "    # Check if the last dimension has five elements\n",
    "    if bounding_boxes_raw.shape[-1] == 5:\n",
    "        # Extract the class ID (last element) and bounding box coordinates\n",
    "        class_ids = bounding_boxes_raw[..., -1]  # The last element is the class ID\n",
    "        bounding_boxes = bounding_boxes_raw[..., :-1]  # The rest is the bounding box coordinates\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected bounding box shape: {bounding_boxes_raw.shape}\")\n",
    "    return bounding_boxes, class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "2d728cb6-0d48-4513-aeeb-f5ab25b53bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize image data\n",
    "def normalize_image_data(image):\n",
    "    # Convert TensorFlow tensor to NumPy array\n",
    "    image = image.numpy()  # Explicit conversion\n",
    "    # If data is in float format, scale to [0, 255]\n",
    "    if image.dtype == np.float32 or image.dtype == np.float64:\n",
    "        image = (image * 255).astype(np.uint8)  # Scale to [0, 255]\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "579c499c-da86-4bfe-a6b9-e1dfd4b3d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert from BGR to RGB if needed\n",
    "def ensure_rgb_format(image):\n",
    "    # If the image appears incorrect, try converting BGR to RGB\n",
    "    if image.shape[-1] == 3:  # Assuming three channels (RGB or BGR)\n",
    "        return image[..., ::-1]  # Reverse the color channels to convert BGR to RGB\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "ca6ada37-47ce-4d27-8138-4027ad1baaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_empty_annotations(data):\n",
    "    image, annotations = data  # Unpack data tuple\n",
    "    # Filter logic based on annotations (assuming annotations is a list)\n",
    "    if not all(value == -1 for value in annotations):\n",
    "        return True  # Keep data if not empty\n",
    "    return False  # Remove data if empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "cafe9096-22f6-41c5-81e4-57ad239480c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_annotations(image, annotations, max_annotations=5):\n",
    "    num_annotations = tf.shape(annotations)[0]\n",
    "    annotations = tf.reshape(annotations, [num_annotations, 5])\n",
    "\n",
    "    padding = [[0, max_annotations - num_annotations], [0, 0]]\n",
    "    annotations = tf.pad(annotations, padding, constant_values=-1)\n",
    "\n",
    "    boxes = annotations[:, :4]\n",
    "    classes = tf.expand_dims(annotations[:, 4], axis=-1)\n",
    "    \n",
    "    return image, {'boxes': boxes, 'classes': classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "c29aff6b-e9b5-4cbe-b980-908f35af6efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader with flexible batch size and error handling\n",
    "def data_loader(images_dir, labels_dir, batch_size):\n",
    "    image_paths = list(Path(images_dir).rglob(\"*.jpg\")) + list(Path(images_dir).rglob(\"*.png\"))\n",
    "    \n",
    "    if len(image_paths) == 0:\n",
    "        raise ValueError(f\"No images found in {images_dir}. Check your dataset path.\")\n",
    "\n",
    "    # Create TensorFlow dataset object from list of image paths\n",
    "    # from_tensor_slices method creates a dataset from the given list\n",
    "    # with each element being a string representing an image path\n",
    "    dataset = tf.data.Dataset.from_tensor_slices([str(p) for p in image_paths])\n",
    "\n",
    "    # Map function to load images and annotations with error handling\n",
    "    dataset = dataset.map(\n",
    "        lambda x: tf.py_function(\n",
    "            lambda y: load_sample(y, labels_dir),\n",
    "            [x],\n",
    "            [tf.float32, tf.float32]\n",
    "        ),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    )\n",
    "\n",
    "    # Apply the resizing function\n",
    "    dataset = dataset.map(\n",
    "        lambda image, annotations: tf.py_function(\n",
    "            func=lambda img, ann: inference_resizing(img, ann),\n",
    "            inp=[image, annotations],\n",
    "            Tout=[tf.float32, tf.float32]\n",
    "        ),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    )\n",
    "\n",
    "    # Normalize image data\n",
    "    dataset = dataset.map(\n",
    "        lambda image, annotations: tf.py_function(\n",
    "            func=lambda img, ann: (normalize_image_data(img), ann),\n",
    "            inp=[image, annotations],\n",
    "            Tout=[tf.float32, tf.float32]\n",
    "        ),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    )\n",
    "\n",
    "\n",
    "    # Ensure RGB format\n",
    "    dataset = dataset.map(\n",
    "        lambda image, annotations: tf.py_function(\n",
    "            func=lambda img, ann: (ensure_rgb_format(img), ann),\n",
    "            inp=[image, annotations],\n",
    "            Tout=[tf.float32, tf.float32]\n",
    "        ),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    )\n",
    "    \n",
    "    # Filter out empty annotations\n",
    "    dataset = dataset.filter(filter_empty_annotations)\n",
    "    \n",
    "    # Pad annotations\n",
    "    dataset = dataset.map(pad_annotations)\n",
    "    \n",
    "    # Apply batching, allowing for partial batches\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "b7b72f97-5958-4bac-9887-a37bd090f3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset(dataset, value_range, default_rows, default_cols, bounding_box_format):\n",
    "    batch = next(iter(dataset.take(1)))\n",
    "\n",
    "    images, bounding_boxes_raw = batch\n",
    "\n",
    "    rows = default_rows\n",
    "    cols = default_cols\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "    axs = axs.flatten() if rows * cols > 1 else [axs]\n",
    "\n",
    "    for ax, image, bboxes in zip(axs, images, bounding_boxes_raw):\n",
    "        ax.imshow(image, vmin=value_range[0], vmax=value_range[1])\n",
    "        for bbox in bboxes:\n",
    "            if tf.reduce_all(bbox != -1):\n",
    "                x_min, y_min, x_max, y_max, _ = bbox\n",
    "                rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='r', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "84af5ea1-f503-408c-8ee4-b89a7be5af3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n\n    TypeError: outer_factory.<locals>.inner_factory.<locals>.tf__filter_empty_annotations() takes 1 positional argument but 2 were given\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[289], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create datasets for training, validation, and testing\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdata_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_IMAGES_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAIN_LABELS_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m data_loader(VAL_IMAGES_DIR, VAL_LABELS_DIR, BATCH_SIZE \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[287], line 55\u001b[0m, in \u001b[0;36mdata_loader\u001b[1;34m(images_dir, labels_dir, batch_size)\u001b[0m\n\u001b[0;32m     45\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m image, annotations: tf\u001b[38;5;241m.\u001b[39mpy_function(\n\u001b[0;32m     47\u001b[0m         func\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m img, ann: (ensure_rgb_format(img), ann),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE,\n\u001b[0;32m     52\u001b[0m )\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Filter out empty annotations\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilter_empty_annotations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Pad annotations\u001b[39;00m\n\u001b[0;32m     58\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(pad_annotations)\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2518\u001b[0m, in \u001b[0;36mDatasetV2.filter\u001b[1;34m(self, predicate, name)\u001b[0m\n\u001b[0;32m   2514\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> filter_op ->\u001b[39;00m\n\u001b[0;32m   2515\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[0;32m   2516\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m   2517\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m filter_op\n\u001b[1;32m-> 2518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfilter_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\filter_op.py:25\u001b[0m, in \u001b[0;36m_filter\u001b[1;34m(input_dataset, predicate, name)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_filter\u001b[39m(input_dataset, predicate, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_FilterDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\filter_op.py:38\u001b[0m, in \u001b[0;36m_FilterDataset.__init__\u001b[1;34m(self, input_dataset, predicate, use_legacy_function, name)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.filter` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dataset \u001b[38;5;241m=\u001b[39m input_dataset\n\u001b[1;32m---> 38\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_legacy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wrapped_func\u001b[38;5;241m.\u001b[39moutput_structure\u001b[38;5;241m.\u001b[39mis_compatible_with(\n\u001b[0;32m     44\u001b[0m     tensor_spec\u001b[38;5;241m.\u001b[39mTensorSpec([], dtypes\u001b[38;5;241m.\u001b[39mbool)):\n\u001b[0;32m     45\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid `predicate`. `predicate` must return a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.bool` scalar tensor, but its return type is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     47\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwrapped_func\u001b[38;5;241m.\u001b[39moutput_structure\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:265\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m    258\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    259\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    260\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    261\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    263\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[1;32m--> 265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[0;32m    267\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1251\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1250\u001b[0m   \u001b[38;5;66;03m# Implements PolymorphicFunction.get_concrete_function.\u001b[39;00m\n\u001b[1;32m-> 1251\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1253\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1221\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1219\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1220\u001b[0m     initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1221\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[0;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m   1225\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m   1226\u001b[0m   \u001b[38;5;66;03m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:696\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[0;32m    692\u001b[0m     variable_capturing_scope,\n\u001b[0;32m    693\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[0;32m    694\u001b[0m )\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    701\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[0;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[0;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[1;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[0;32m    290\u001b[0m   )\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[1;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[0;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[0;32m    304\u001b[0m       placeholder_context\n\u001b[0;32m    305\u001b[0m   )\n\u001b[0;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    309\u001b[0m )\n\u001b[1;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_type_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[0;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1059\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[0;32m   1056\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m   1058\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1059\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:599\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    596\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 599\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:231\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):  \u001b[38;5;66;03m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m   ret \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_structure, ret)\n\u001b[0;32m    233\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ret]\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:161\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[0;32m    160\u001b[0m   nested_args \u001b[38;5;241m=\u001b[39m (nested_args,)\n\u001b[1;32m--> 161\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mautograph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_ctx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnested_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m ret \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(ret)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_pack(ret):\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:693\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m    694\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    695\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconverted_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meffective_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args)\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n\n    TypeError: outer_factory.<locals>.inner_factory.<locals>.tf__filter_empty_annotations() takes 1 positional argument but 2 were given\n"
     ]
    }
   ],
   "source": [
    "# Create datasets for training, validation, and testing\n",
    "train_dataset = data_loader(TRAIN_IMAGES_DIR, TRAIN_LABELS_DIR, BATCH_SIZE)\n",
    "val_dataset = data_loader(VAL_IMAGES_DIR, VAL_LABELS_DIR, BATCH_SIZE // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33b5946-d749-42dc-8061-0c791f1ddf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, annotations in train_dataset.take(1):\n",
    "    print(\"Image shape:\", image.shape)\n",
    "    print(\"Boxes shape:\", annotations['boxes'].shape)\n",
    "    print(\"Classes shape:\", annotations['classes'].shape)\n",
    "    print(\"Boxes:\", annotations['boxes'])\n",
    "    print(\"Classes:\", annotations['classes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0a006c-f8e3-46e0-8060-81f67b7b0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "visualize_dataset(train_dataset, value_range=(0, 1), default_rows=1, default_cols=1, bounding_box_format=\"xyxy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "6f7dc7ef-82e3-4a7b-9928-9bf57e9ed6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (640, 640, 3), Annotations: [[ 10.98048 174.64032 155.29408 103.52928   0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[ 37.64736   2.09184 560.      373.33344   0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[172.5488    6.27424 401.56833 602.35297   0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[189.28064   1.57632 385.88223 581.67487   0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[180.91489  10.98016 611.7648  917.64703   0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[ 70.41248   5.664   632.456   949.61664   0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[104.3616   45.44416 250.21632 376.8048    0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[2.2274529e+02 3.1999999e-04 4.8941217e+02 6.8383582e+02 0.0000000e+00]]\n",
      "Image shape: (640, 640, 3), Annotations: [[100.68992  35.55584 477.2416  361.8304    0.     ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[286], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvisualize_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounding_box_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxywh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[284], line 16\u001b[0m, in \u001b[0;36mvisualize_dataset\u001b[1;34m(dataset, value_range, default_rows, default_cols, bounding_box_format)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bbox \u001b[38;5;129;01min\u001b[39;00m bboxes:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mreduce_all(bbox \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 16\u001b[0m         x_min, y_min, x_max, y_max, _ \u001b[38;5;241m=\u001b[39m bbox\n\u001b[0;32m     17\u001b[0m         rect \u001b[38;5;241m=\u001b[39m patches\u001b[38;5;241m.\u001b[39mRectangle((x_min, y_min), x_max \u001b[38;5;241m-\u001b[39m x_min, y_max \u001b[38;5;241m-\u001b[39m y_min, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m         ax\u001b[38;5;241m.\u001b[39madd_patch(rect)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 1)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_dataset(val_dataset, value_range=(0, 1), default_rows=1, default_cols=1, bounding_box_format=\"xywh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0be0e4-f1d0-4f7e-b3cc-be88d50c37b0",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "1282ed9d-c3f8-4c92-a181-77093e3d1c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (640, 640, 3), Annotations: [[179.3936    3.87872 432.4848  864.9699    0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[ 22.63264 113.6096  226.32608 340.8288    0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[ 24.66016   4.66624 573.064   325.33313   0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[219.60768  89.41184 387.97375 581.96094   0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[ 95.68608  15.6864  487.84286 325.2288    0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[ 21.96096   7.32    607.0592  404.7056    0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[ 78.43136   7.32    619.60767 413.0717    0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[ 20.9152  130.83711 215.42528 324.72897   0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[  6.27456   3.09952 638.9542  946.8282    0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[269.0003   26.66688 394.      700.4448    0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[138.07584  94.11744 557.9011  312.67935   0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[170.98016  15.6864  495.68607 330.4576    0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[169.41151  15.68672 564.7056  376.47073   0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[353.464   185.09824 437.12415 655.6864    0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[ 21.96096 140.13055 249.41183 166.27457   0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[ 79.328    10.4576  588.90753 458.03903   0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[158.95456   6.28992 542.7453  816.11774   0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[314.24    45.0976 599.04   898.1114   0.    ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[109.80416  41.82976 580.39233 386.928     0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[ 22.85728  10.45728 610.6122  391.11072   0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[220.65376  12.54912 430.8496  646.27454   0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[366.01312 197.64703 446.536   669.80383   0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[234.79263   1.89376 459.48703 689.2307    0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[ 22.85696   3.13696 636.73474 407.84286   0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[193.18529   5.33344 480.      720.0003    0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[ 44.61536  13.59456 613.84607 417.25473   0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[  6.27456  12.8     624.3136  955.2       0.     ]]\n",
      "Image shape: (640, 640, 3), Annotations: [[ 23.00672  47.05888 160.      240.00032   0.     ]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes at component 0: expected [?,224,224,3] but got [3,640,640,3]. [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[224], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mImage shape:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAnnotations:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:809\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    808\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:772\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 772\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    777\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3086\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3084\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3085\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 3086\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3087\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   3088\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5983\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   5981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   5982\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 5983\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes at component 0: expected [?,224,224,3] but got [3,640,640,3]. [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "for image, annotations in train_dataset.take(1):\n",
    "    print(\"Image shape:\", image.shape)\n",
    "    print(\"Annotations:\", annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "919c4499-2177-4809-9412-7b66b3d7eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = 0.005\n",
    "# including a global_clipnorm is extremely important in object detection tasks\n",
    "optimizer = keras.optimizers.SGD(\n",
    "    learning_rate=base_lr, momentum=0.9, global_clipnorm=10.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c18a0f0b-f9c4-46ef-8baf-39b78aa966de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = keras_cv.models.YOLOV8Detector.from_preset(\n",
    "    \"yolo_v8_m_pascalvoc\", bounding_box_format=\"xywh\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b965a862-00b0-457c-9b34-1d9846ca4e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.compile(\n",
    "    classification_loss=\"binary_crossentropy\",\n",
    "    box_loss=\"ciou\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "d97e39ce-151a-43a3-8b1a-5a66af120b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_metrics_callback = keras_cv.callbacks.PyCOCOCallback(\n",
    "    val_dataset.take(20), bounding_box_format=\"xywh\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "9b603f09-42ef-4843-b031-d5295d552b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras_cv.models.YOLOV8Detector.from_preset(\n",
    "    \"resnet50_imagenet\",\n",
    "    # For more info on supported bounding box formats, visit\n",
    "    # https://keras.io/api/keras_cv/bounding_box/\n",
    "    bounding_box_format=\"xywh\",\n",
    "    num_classes=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "b5c5e1b5-907f-414f-8f06-f5518073da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    classification_loss=\"binary_crossentropy\",\n",
    "    box_loss=\"ciou\",\n",
    "    optimizer=optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9972375a-5281-41e3-9bbc-9061eafcd177",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Cond.call().\n\n\u001b[1mDimension must be 4 but is 3 for '{{node yolov8_label_encoder_7_1/cond/transpose}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](yolov8_label_encoder_7_1/cond/Reshape_2, yolov8_label_encoder_7_1/cond/transpose/perm)' with input shapes: [?,?,?,?], [3].\u001b[0m\n\nArguments received by Cond.call():\n  • args=('tf.Tensor(shape=(), dtype=bool)', '<function YOLOV8LabelEncoder.call.<locals>.<lambda> at 0x00000169FC101300>', '<function YOLOV8LabelEncoder.call.<locals>.<lambda> at 0x00000169FC101260>')\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[231], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Run for 10-35~ epochs to achieve good scores.\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcoco_metrics_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\keras_cv\\src\\models\\object_detection\\yolo_v8\\yolo_v8_detector.py:526\u001b[0m, in \u001b[0;36mYOLOV8Detector.train_step\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    524\u001b[0m args \u001b[38;5;241m=\u001b[39m args[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    525\u001b[0m x, y \u001b[38;5;241m=\u001b[39m unpack_input(data)\n\u001b[1;32m--> 526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\keras_cv\\src\\models\\object_detection\\yolo_v8\\yolo_v8_detector.py:555\u001b[0m, in \u001b[0;36mYOLOV8Detector.compute_loss\u001b[1;34m(self, x, y, y_pred, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    546\u001b[0m gt_bboxes \u001b[38;5;241m=\u001b[39m bounding_box\u001b[38;5;241m.\u001b[39mconvert_format(\n\u001b[0;32m    547\u001b[0m     y[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    548\u001b[0m     source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbounding_box_format,\n\u001b[0;32m    549\u001b[0m     target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxyxy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    550\u001b[0m     images\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m    551\u001b[0m )\n\u001b[0;32m    553\u001b[0m pred_bboxes \u001b[38;5;241m=\u001b[39m dist2bbox(pred_boxes, anchor_points)\n\u001b[1;32m--> 555\u001b[0m target_bboxes, target_scores, fg_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_bboxes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43manchor_points\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_gt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m target_bboxes \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m stride_tensor\n\u001b[0;32m    565\u001b[0m target_scores_sum \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mmaximum(ops\u001b[38;5;241m.\u001b[39msum(target_scores), \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\keras_cv\\src\\models\\object_detection\\yolo_v8\\yolo_v8_label_encoder.py:248\u001b[0m, in \u001b[0;36mYOLOV8LabelEncoder.call\u001b[1;34m(self, scores, decode_bboxes, anchors, gt_labels, gt_bboxes, gt_mask)\u001b[0m\n\u001b[0;32m    244\u001b[0m max_num_boxes \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mshape(gt_bboxes)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# If there are no GT boxes in the batch, we short-circuit and return\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# empty targets to avoid NaNs.\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_num_boxes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_mask\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecode_bboxes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\keras_cv\\src\\models\\object_detection\\yolo_v8\\yolo_v8_label_encoder.py:250\u001b[0m, in \u001b[0;36mYOLOV8LabelEncoder.call.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    244\u001b[0m max_num_boxes \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mshape(gt_bboxes)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# If there are no GT boxes in the batch, we short-circuit and return\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# empty targets to avoid NaNs.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcond(\n\u001b[0;32m    249\u001b[0m     ops\u001b[38;5;241m.\u001b[39marray(max_num_boxes \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_mask\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[0;32m    254\u001b[0m         ops\u001b[38;5;241m.\u001b[39mzeros_like(decode_bboxes),\n\u001b[0;32m    255\u001b[0m         ops\u001b[38;5;241m.\u001b[39mzeros_like(scores),\n\u001b[0;32m    256\u001b[0m         ops\u001b[38;5;241m.\u001b[39mzeros_like(scores[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m]),\n\u001b[0;32m    257\u001b[0m     ),\n\u001b[0;32m    258\u001b[0m )\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\keras_cv\\src\\models\\object_detection\\yolo_v8\\yolo_v8_label_encoder.py:97\u001b[0m, in \u001b[0;36mYOLOV8LabelEncoder.assign\u001b[1;34m(self, scores, decode_bboxes, anchors, gt_labels, gt_bboxes, gt_mask)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Box scores are the predicted scores for each anchor, ground truth box\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# pair. Only the predicted score for the class of the GT box is included\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Shape: (B, num_gt_boxes, num_anchors) (after transpose)\u001b[39;00m\n\u001b[0;32m     92\u001b[0m bbox_scores \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mtake_along_axis(\n\u001b[0;32m     93\u001b[0m     scores,\n\u001b[0;32m     94\u001b[0m     ops\u001b[38;5;241m.\u001b[39mcast(ops\u001b[38;5;241m.\u001b[39mmaximum(gt_labels[:, \u001b[38;5;28;01mNone\u001b[39;00m, :], \u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     95\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     96\u001b[0m )\n\u001b[1;32m---> 97\u001b[0m bbox_scores \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Overlaps are the IoUs of each predicted box and each GT box.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Shape: (B, num_gt_boxes, num_anchors)\u001b[39;00m\n\u001b[0;32m    101\u001b[0m overlaps \u001b[38;5;241m=\u001b[39m compute_ciou(\n\u001b[0;32m    102\u001b[0m     ops\u001b[38;5;241m.\u001b[39mexpand_dims(gt_bboxes, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m    103\u001b[0m     ops\u001b[38;5;241m.\u001b[39mexpand_dims(decode_bboxes, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    104\u001b[0m     bounding_box_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxyxy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    105\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Cond.call().\n\n\u001b[1mDimension must be 4 but is 3 for '{{node yolov8_label_encoder_7_1/cond/transpose}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](yolov8_label_encoder_7_1/cond/Reshape_2, yolov8_label_encoder_7_1/cond/transpose/perm)' with input shapes: [?,?,?,?], [3].\u001b[0m\n\nArguments received by Cond.call():\n  • args=('tf.Tensor(shape=(), dtype=bool)', '<function YOLOV8LabelEncoder.call.<locals>.<lambda> at 0x00000169FC101300>', '<function YOLOV8LabelEncoder.call.<locals>.<lambda> at 0x00000169FC101260>')\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset.take(20),\n",
    "    # Run for 10-35~ epochs to achieve good scores.\n",
    "    epochs=1,\n",
    "    callbacks=[coco_metrics_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c87d7-5e2d-4202-9b5a-39f8c608d9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
