{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2415e9d8-b68f-47b7-b7b1-09632788855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "import logging  # To log errors or missing files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bb30dc-9c93-4611-9d3a-d3bf8a1ba506",
   "metadata": {},
   "source": [
    "## Images rename to image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "daf9945d-6251-40d5-b29a-1ef1406d3e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Projects\\\\DL\\\\MonumentDetection\\\\DL\\\\keras_cv'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f891ba62-378c-450d-bd25-6d5d6d540a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('coco-nyaptola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c459b358-d8d0-4307-8a9c-2375a6276ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Projects\\\\DL\\\\MonumentDetection\\\\DL\\\\keras_cv\\\\coco-nyaptola'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32e5427a-527a-4a89-865a-d81682ce18e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "image_dir = 'images'\n",
    "for file in os.listdir(image_dir):\n",
    "    old_filename = os.path.join(image_dir, file)\n",
    "    new_filename = os.path.join(image_dir, str(count) + '.png')\n",
    "    os.rename(old_filename, new_filename)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df3c78d-0864-4bcc-b858-9444eed83e97",
   "metadata": {},
   "source": [
    "## Keras_CV Compatible Annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4a26a95-db71-491c-bd39-b037bbf61337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_convert_annotations(annotations_file, output_file):\n",
    "  with open(annotations_file, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "  xywh_annotations = convert_coco_to_xywh(annotations)\n",
    "\n",
    "  with open(output_file, 'w') as f:\n",
    "    json.dump(xywh_annotations, f, indent=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96d16172-c09b-4a19-a970-76bf81ac8bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_coco_to_xywh(annotations):\n",
    "  #Converts bounding boxes in COCO format to xywh format.\n",
    "  xywh_annotations = []\n",
    "  for ann in annotations[\"annotations\"]:\n",
    "    image_id = ann[\"image_id\"]\n",
    "    bbox = ann[\"bbox\"]  # Assuming bbox is in [xmin, ymin, width, height] format\n",
    "    x_min, y_min, width, height = bbox\n",
    "\n",
    "    # Convert to xywh format\n",
    "    x_center = x_min + width / 2\n",
    "    y_center = y_min + height / 2\n",
    "\n",
    "    xywh_annotations.append({\n",
    "      \"image_id\": image_id,\n",
    "      \"bbox\": [x_center, y_center, width, height],\n",
    "      \"category_id\": ann[\"category_id\"],  # Assuming category_id exists\n",
    "    })\n",
    "  return xywh_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3d63980-7acf-4060-bec4-4d6b1efb2bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = \"result.json\"\n",
    "annotation_file = os.path.abspath(annotation_file)\n",
    "output_file = \"annotations.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db1826a9-040a-42c5-b59a-0afd8cd0a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_annotations = load_and_convert_annotations(annotation_file,output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124cfa78-aa8c-4a91-8dc5-a64844d87991",
   "metadata": {},
   "source": [
    "## Conversion of annotations to yolo format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70c7c7db-0af6-4a55-b402-e5227c7f4d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xmltodict\n",
      "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: xmltodict\n",
      "Successfully installed xmltodict-0.13.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\acer\\anaconda3\\envs\\object_detection\\lib\\site-packages\\avro_python3-1.10.2-py3.9.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install xmltodict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9335602c-2e39-475e-a00e-573bb3cc2780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "import os\n",
    "\n",
    "# Function to convert XML to YOLO format\n",
    "def convert_annotation(xml_file):\n",
    "    with open(xml_file, 'r') as file:\n",
    "        annotation = xmltodict.parse(file.read())['annotation']\n",
    "    \n",
    "    yolo_annotations = []\n",
    "    \n",
    "    # Check if 'object' is a list or a single item\n",
    "    objects = annotation.get('object', [])\n",
    "    \n",
    "    if isinstance(objects, dict):\n",
    "        # If it's a dictionary, convert it to a list for consistent processing\n",
    "        objects = [objects]\n",
    "\n",
    "    for obj in objects:\n",
    "        class_name = obj['name']\n",
    "        bbox = obj['bndbox']\n",
    "        \n",
    "        xmin = int(bbox['xmin'])\n",
    "        ymin = int(bbox['ymin'])\n",
    "        xmax = int(bbox['xmax'])\n",
    "        ymax = int(bbox['ymax'])\n",
    "        \n",
    "        # Calculate YOLO format\n",
    "        image_width = int(annotation['size']['width'])\n",
    "        image_height = int(annotation['size']['height'])\n",
    "\n",
    "        x_center = (xmin + xmax) / 2 / image_width\n",
    "        y_center = (ymin + ymax) / 2 / image_height\n",
    "        width = (xmax - xmin) / image_width\n",
    "        height = (ymax - ymin) / image_height\n",
    "        \n",
    "        class_id = 0  # Adjust according to your class mapping\n",
    "        yolo_annotations.append(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "\n",
    "    return yolo_annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2853b0fb-ce83-4462-bbf3-0a430eafd7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all XML annotation files\n",
    "for annotation_file in os.listdir(annotations_path):\n",
    "    if annotation_file.endswith('.xml'):\n",
    "        xml_path = os.path.join(annotations_path, annotation_file)\n",
    "        \n",
    "        # Convert XML to YOLO format\n",
    "        yolo_annotations = convert_annotation(xml_path)\n",
    "        \n",
    "        # Create corresponding text file for YOLO\n",
    "        txt_filename = annotation_file.replace('.xml', '.txt')\n",
    "        txt_path = os.path.join(output_path, txt_filename)\n",
    "        \n",
    "        with open(txt_path, 'w') as txt_file:\n",
    "            txt_file.write('\\n'.join(yolo_annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0b2a137-1ce1-4818-a89c-068644b0ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Set paths\n",
    "images_path = \"dataset/images/\"\n",
    "yolo_labels_path = \"dataset/yolo_labels/\"\n",
    "\n",
    "# Create directories for training and validation\n",
    "os.makedirs(\"dataset/train/images\", exist_ok=True)\n",
    "os.makedirs(\"dataset/train/labels\", exist_ok=True)\n",
    "os.makedirs(\"dataset/val/images\", exist_ok=True)\n",
    "os.makedirs(\"dataset/val/labels\", exist_ok=True)\n",
    "\n",
    "# List all images\n",
    "images = os.listdir(images_path)\n",
    "\n",
    "# Shuffle images and split into training and validation\n",
    "random.shuffle(images)\n",
    "train_ratio = 0.8\n",
    "num_train = int(len(images) * train_ratio)\n",
    "\n",
    "train_images = images[:num_train]\n",
    "val_images = images[num_train:]\n",
    "\n",
    "# Move images and corresponding labels to respective directories\n",
    "for image in train_images:\n",
    "    base_name = os.path.splitext(image)[0]\n",
    "    label_file = f\"{base_name}.txt\"\n",
    "    \n",
    "    shutil.copy(os.path.join(images_path, image), \"dataset/train/images/\")\n",
    "    shutil.copy(os.path.join(yolo_labels_path, label_file), \"dataset/train/labels/\")\n",
    "\n",
    "for image in val_images:\n",
    "    base_name = os.path.splitext(image)[0]\n",
    "    label_file = f\"{base_name}.txt\"\n",
    "    \n",
    "    shutil.copy(os.path.join(images_path, image), \"dataset/val/images/\")\n",
    "    shutil.copy(os.path.join(yolo_labels_path, label_file), \"dataset/val/labels/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ed17ec-fe15-459a-835f-c5ec0397158e",
   "metadata": {},
   "source": [
    "## Another Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb3ee29c-9c17-4bce-89cb-ebd1ae0b1caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched image files:\n",
      "dataset\\train\\images\\55c04d26-Nyathpola.jpg\n",
      "Matched label files:\n",
      "dataset\\train\\labels\\115e44a7-image_29.txt\n",
      "dataset\\train\\labels\\14a405f1-image_10.txt\n",
      "dataset\\train\\labels\\159954d7-image_199.txt\n",
      "dataset\\train\\labels\\174afc0c-image_86.txt\n",
      "dataset\\train\\labels\\1c2822e4-image_480.txt\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: dataset/val/images\\\\*.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Check the validation dataset paths and files\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     val_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdata_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset/val/images\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset/val/labels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m(e)\n",
      "Cell \u001b[1;32mIn[11], line 38\u001b[0m, in \u001b[0;36mdata_loader\u001b[1;34m(images_dir, labels_dir, batch_size)\u001b[0m\n\u001b[0;32m     35\u001b[0m image_files_pattern \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(images_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m label_files_pattern \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(labels_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m image_files \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_files_pattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m label_files \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mlist_files(label_files_pattern, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Print the matched files for debugging\u001b[39;00m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1320\u001b[0m, in \u001b[0;36mDatasetV2.list_files\u001b[1;34m(file_pattern, shuffle, seed, name)\u001b[0m\n\u001b[0;32m   1313\u001b[0m condition \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39mgreater(array_ops\u001b[38;5;241m.\u001b[39mshape(matching_files)[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   1314\u001b[0m                              name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_not_empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1316\u001b[0m message \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo files matched pattern: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1318\u001b[0m     string_ops\u001b[38;5;241m.\u001b[39mreduce_join(file_pattern, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1320\u001b[0m assert_not_empty \u001b[38;5;241m=\u001b[39m \u001b[43mcontrol_flow_assert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAssert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massert_not_empty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcontrol_dependencies([assert_not_empty]):\n\u001b[0;32m   1323\u001b[0m   matching_files \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39midentity(matching_files)\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\tensorflow\\python\\ops\\control_flow_assert.py:102\u001b[0m, in \u001b[0;36mAssert\u001b[1;34m(condition, data, summarize, name)\u001b[0m\n\u001b[0;32m    100\u001b[0m     xs \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_n_to_tensor(data)\n\u001b[0;32m    101\u001b[0m     data_str \u001b[38;5;241m=\u001b[39m [_summarize_eager(x, summarize) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m xs]\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError(\n\u001b[0;32m    103\u001b[0m         node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    104\u001b[0m         op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    105\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to be true. Summarized data: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    106\u001b[0m         (condition, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(data_str)))\n\u001b[0;32m    107\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssert\u001b[39m\u001b[38;5;124m\"\u001b[39m, [condition, data]) \u001b[38;5;28;01mas\u001b[39;00m name:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: dataset/val/images\\\\*.jpg'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "def load_sample_with_shape(image_path, label_path):\n",
    "    # Load the image\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [256, 256])  # Resize or reshape as necessary\n",
    "\n",
    "    # Load the annotations\n",
    "    label = tf.io.read_file(label_path)\n",
    "    label = tf.strings.split(label, sep='\\n')\n",
    "    label = tf.strings.to_number(label, tf.float32)\n",
    "    label = tf.reshape(label, [-1, 4])  # Assuming each label has 4 values (e.g., bounding box coordinates)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "def filter_empty_annotations(image, annotations):\n",
    "    # Filter out samples where annotations are empty\n",
    "    return tf.size(annotations) > 0\n",
    "\n",
    "def pad_annotations(image, annotations, max_num_annotations=100):\n",
    "    num_annotations = tf.shape(annotations)[0]\n",
    "    pad_size = max_num_annotations - num_annotations\n",
    "\n",
    "    # Ensure pad_size is non-negative\n",
    "    pad_size = tf.maximum(pad_size, 0)\n",
    "\n",
    "    # Create padding tensor dynamically\n",
    "    padding = tf.stack([[0, pad_size], [0, 0]], axis=0)\n",
    "    padded_annotations = tf.pad(annotations, padding, constant_values=-1)\n",
    "    return image, padded_annotations\n",
    "\n",
    "def data_loader(images_dir, labels_dir, batch_size):\n",
    "    image_files_pattern = os.path.join(images_dir, \"*.jpg\")\n",
    "    label_files_pattern = os.path.join(labels_dir, \"*.txt\")\n",
    "\n",
    "    image_files = tf.data.Dataset.list_files(image_files_pattern, shuffle=False)\n",
    "    label_files = tf.data.Dataset.list_files(label_files_pattern, shuffle=False)\n",
    "\n",
    "    # Print the matched files for debugging\n",
    "    tf.print(\"Matched image files:\")\n",
    "    for file in image_files.take(5):\n",
    "        tf.print(file)\n",
    "    tf.print(\"Matched label files:\")\n",
    "    for file in label_files.take(5):\n",
    "        tf.print(file)\n",
    "\n",
    "    # Check if the datasets are empty\n",
    "    if tf.data.experimental.cardinality(image_files) == 0:\n",
    "        raise FileNotFoundError(f\"No image files found in {images_dir} with pattern {image_files_pattern}\")\n",
    "    if tf.data.experimental.cardinality(label_files) == 0:\n",
    "        raise FileNotFoundError(f\"No label files found in {labels_dir} with pattern {label_files_pattern}\")\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((image_files, label_files))\n",
    "\n",
    "    dataset = dataset.map(load_sample_with_shape, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.filter(lambda image, annotations: tf.py_function(\n",
    "        func=filter_empty_annotations,\n",
    "        inp=[image, annotations],\n",
    "        Tout=tf.bool)\n",
    "    )\n",
    "\n",
    "    dataset = dataset.map(lambda image, annotations: pad_annotations(image, annotations), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Usage\n",
    "train_dataset = data_loader(\"dataset/train/images\", \"dataset/train/labels\", batch_size=3)\n",
    "\n",
    "# Check the validation dataset paths and files\n",
    "try:\n",
    "    val_dataset = data_loader(\"dataset/val/images\", \"dataset/val/labels\", batch_size=3)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "004adb74-ae5a-45ee-89e2-631e0bd93ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Projects\\\\DL\\\\MonumentDetection\\\\DL'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77cb5f8-15e3-4200-8d0f-da133228a876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
