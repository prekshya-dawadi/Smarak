{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22bb1e47-2335-4494-b207-d13c08ed2cbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 64 and 2 for '{{node add}} = AddV2[T=DT_FLOAT](yolov8_detector_1/box_1/concat/concat, concat)' with input shapes: [?,?,64], [8400,2].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 399\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;66;03m# Compile and fit the model\u001b[39;00m\n\u001b[0;32m    391\u001b[0m yolo\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m    392\u001b[0m \n\u001b[0;32m    393\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer, classification_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, box_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mciou\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    394\u001b[0m \n\u001b[0;32m    395\u001b[0m )\n\u001b[1;32m--> 399\u001b[0m \u001b[43myolo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \n\u001b[0;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \n\u001b[0;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCH\u001b[49m\n\u001b[0;32m    406\u001b[0m \n\u001b[0;32m    407\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\miniconda\\envs\\object_detection\\Lib\\site-packages\\keras_cv\\src\\models\\object_detection\\yolo_v8\\yolo_v8_detector.py:526\u001b[0m, in \u001b[0;36mYOLOV8Detector.train_step\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    524\u001b[0m args \u001b[38;5;241m=\u001b[39m args[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    525\u001b[0m x, y \u001b[38;5;241m=\u001b[39m unpack_input(data)\n\u001b[1;32m--> 526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 307\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(self, x, y, y_pred, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m mask_gt \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_all(y[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    295\u001b[0m gt_bboxes \u001b[38;5;241m=\u001b[39m bounding_box\u001b[38;5;241m.\u001b[39mconvert_format(\n\u001b[0;32m    296\u001b[0m \n\u001b[0;32m    297\u001b[0m     y[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m \n\u001b[0;32m    305\u001b[0m )\n\u001b[1;32m--> 307\u001b[0m pred_bboxes \u001b[38;5;241m=\u001b[39m \u001b[43mdist2bbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchor_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape pred_scores: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf\u001b[38;5;241m.\u001b[39mshape(pred_scores)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape pred_boxes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf\u001b[38;5;241m.\u001b[39mshape(pred_boxes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 243\u001b[0m, in \u001b[0;36mdist2bbox\u001b[1;34m(pred_boxes, anchor_points)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdist2bbox\u001b[39m(pred_boxes, anchor_points):\n\u001b[0;32m    240\u001b[0m \n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# Assuming pred_boxes are offsets, convert them to absolute positions using anchor_points\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpred_boxes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43manchor_points\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions must be equal, but are 64 and 2 for '{{node add}} = AddV2[T=DT_FLOAT](yolov8_detector_1/box_1/concat/concat, concat)' with input shapes: [?,?,64], [8400,2]."
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import keras_cv\n",
    "\n",
    "import cv2\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from keras_cv import models, bounding_box, visualization, losses, callbacks\n",
    "\n",
    "from keras_cv.models import YOLOV8Detector\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "\n",
    "# Define constants\n",
    "\n",
    "IMAGE_SIZE = (640, 640)\n",
    "\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "BOUNDING_BOX_FORMAT = \"xywh\"\n",
    "\n",
    "PAD_TO_ASPECT_RATIO = True\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "EPOCH = 1\n",
    "\n",
    "GLOBAL_CLIPNORM = 10.0\n",
    "\n",
    "TRAIN_IMAGES_DIR = Path(\"D:/Projects/DL/MonumentDetection/DL/train/images\")\n",
    "TRAIN_LABELS_DIR = Path(\"D:/Projects/DL/MonumentDetection/DL/train/labels\")\n",
    "\n",
    "VAL_IMAGES_DIR = Path(\"D:/Projects/DL/MonumentDetection/DL/val/images\")\n",
    "VAL_LABELS_DIR = Path(\"D:/Projects/DL/MonumentDetection/DL/val/labels\")\n",
    "\n",
    "\n",
    "\n",
    "def load_yolo_annotations(label_path, image_size):\n",
    "\n",
    "    annotations = []\n",
    "\n",
    "    with open(label_path, 'r') as file:\n",
    "\n",
    "        for line in file:\n",
    "\n",
    "            parts = line.strip().split(\" \")\n",
    "\n",
    "            if len(parts) != 5:\n",
    "\n",
    "                continue\n",
    "\n",
    "            class_id = int(parts[0])\n",
    "\n",
    "            x_center = float(parts[1])\n",
    "\n",
    "            y_center = float(parts[2])\n",
    "\n",
    "            width = float(parts[3])\n",
    "\n",
    "            height = float(parts[4])\n",
    "\n",
    "            x_min = (x_center - width / 2) * image_size[0]\n",
    "\n",
    "            y_min = (y_center - height / 2) * image_size[1]\n",
    "\n",
    "            x_max = (x_center + width / 2) * image_size[0]\n",
    "\n",
    "            y_max = (y_center + height / 2) * image_size[1]\n",
    "\n",
    "            annotations.append([x_min, y_min, x_max, y_max, class_id])\n",
    "\n",
    "    return np.array(annotations, dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "def load_sample(image_path, labels_dir):\n",
    "\n",
    "    image_path_str = tf.keras.backend.get_value(image_path).decode(\"utf-8\")\n",
    "\n",
    "    image = Image.open(image_path_str).resize(IMAGE_SIZE)\n",
    "\n",
    "    image = np.array(image) / 255.0\n",
    "\n",
    "    image_stem = Path(image_path_str).stem\n",
    "\n",
    "    label_path = os.path.join(labels_dir, image_stem + \".txt\")\n",
    "\n",
    "    if not os.path.isfile(label_path):\n",
    "\n",
    "        raise FileNotFoundError(f\"Label file not found: {label_path}\")\n",
    "\n",
    "    annotations = load_yolo_annotations(label_path, IMAGE_SIZE)\n",
    "\n",
    "    return image, annotations\n",
    "\n",
    "\n",
    "\n",
    "def filter_empty_annotations(image, annotations):\n",
    "\n",
    "    return tf.size(annotations) > 0\n",
    "\n",
    "\n",
    "\n",
    "def pad_annotations(image, annotations, max_annotations=5):\n",
    "\n",
    "    num_annotations = tf.shape(annotations)[0]\n",
    "\n",
    "    annotations = tf.reshape(annotations, [num_annotations, 5])\n",
    "\n",
    "    padding = [[0, max_annotations - num_annotations], [0, 0]]\n",
    "\n",
    "    annotations = tf.pad(annotations, padding, constant_values=-1)\n",
    "\n",
    "    boxes = annotations[:, :4]\n",
    "\n",
    "    classes = tf.expand_dims(annotations[:, 4], axis=-1)\n",
    "\n",
    "    return image, {'boxes': boxes, 'classes': classes}\n",
    "\n",
    "\n",
    "\n",
    "def data_loader(images_dir, labels_dir, batch_size):\n",
    "\n",
    "    image_paths = list(Path(images_dir).rglob(\"*.jpg\")) + list(Path(images_dir).rglob(\"*.png\"))\n",
    "\n",
    "    if len(image_paths) == 0:\n",
    "\n",
    "        raise ValueError(f\"No images found in {images_dir}. Check your dataset path.\")\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices([str(p) for p in image_paths])\n",
    "\n",
    "    def load_sample_with_shape(image_path):\n",
    "\n",
    "        image, annotations = tf.py_function(\n",
    "\n",
    "            lambda y: load_sample(y, labels_dir),\n",
    "\n",
    "            [image_path],\n",
    "\n",
    "            [tf.float32, tf.float32]\n",
    "\n",
    "        )\n",
    "\n",
    "        image.set_shape(IMAGE_SIZE + (3,))\n",
    "\n",
    "        annotations.set_shape([None, 5])\n",
    "\n",
    "        return image, annotations\n",
    "\n",
    "    dataset = dataset.map(load_sample_with_shape, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.filter(lambda image, annotations: tf.py_function(\n",
    "\n",
    "        func=filter_empty_annotations,\n",
    "\n",
    "        inp=[image, annotations],\n",
    "\n",
    "        Tout=tf.bool)\n",
    "\n",
    "    )\n",
    "\n",
    "    dataset = dataset.map(lambda image, annotations: pad_annotations(image, annotations))\n",
    "\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "# Create datasets for training and validation\n",
    "\n",
    "train_dataset = data_loader(TRAIN_IMAGES_DIR, TRAIN_LABELS_DIR, BATCH_SIZE)\n",
    "\n",
    "val_dataset = data_loader(VAL_IMAGES_DIR, VAL_LABELS_DIR, BATCH_SIZE // 2)\n",
    "\n",
    "\n",
    "\n",
    "class_ids = [\"Nyatapola\"]\n",
    "\n",
    "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
    "\n",
    "\n",
    "\n",
    "backbone = keras_cv.models.YOLOV8Backbone.from_preset(\n",
    "\n",
    "    \"yolo_v8_s_backbone_coco\"\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "\n",
    "    learning_rate=LEARNING_RATE,\n",
    "\n",
    "    global_clipnorm=GLOBAL_CLIPNORM,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "yolo = keras_cv.models.YOLOV8Detector(\n",
    "\n",
    "    num_classes=len(class_mapping),\n",
    "\n",
    "    bounding_box_format=\"xyxy\",\n",
    "\n",
    "    backbone=backbone,\n",
    "\n",
    "    fpn_depth=1,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Realistic YOLO function replacements\n",
    "\n",
    "def decode_regression_to_boxes(box_pred):\n",
    "\n",
    "    # Assuming box_pred is of shape [batch_size, num_boxes, 4]\n",
    "\n",
    "    return box_pred\n",
    "\n",
    "\n",
    "\n",
    "def dist2bbox(pred_boxes, anchor_points):\n",
    "\n",
    "    # Assuming pred_boxes are offsets, convert them to absolute positions using anchor_points\n",
    "\n",
    "    return pred_boxes + anchor_points\n",
    "\n",
    "\n",
    "\n",
    "def get_anchors(image_shape):\n",
    "\n",
    "    height, width = image_shape[:2]\n",
    "\n",
    "    strides = [8, 16, 32]  # Strides for different layers\n",
    "\n",
    "    anchors = []\n",
    "\n",
    "    stride_tensors = []\n",
    "\n",
    "    for stride in strides:\n",
    "\n",
    "        grid_x, grid_y = tf.meshgrid(tf.range(width // stride), tf.range(height // stride))\n",
    "\n",
    "        anchor_points = tf.stack([grid_x, grid_y], axis=-1)\n",
    "\n",
    "        anchor_points = tf.cast(anchor_points, dtype=tf.float32)\n",
    "\n",
    "        anchor_points = tf.reshape(anchor_points, [-1, 2]) * stride\n",
    "\n",
    "        anchors.append(anchor_points)\n",
    "\n",
    "        stride_tensors.append(tf.fill([tf.shape(anchor_points)[0]], stride))\n",
    "\n",
    "        \n",
    "\n",
    "    return tf.concat(anchors, axis=0), tf.concat(stride_tensors, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# Debugging function addition to compute_loss method\n",
    "\n",
    "def compute_loss(self, x, y, y_pred, sample_weight=None, **kwargs):\n",
    "\n",
    "    box_pred, cls_pred = y_pred[\"boxes\"], y_pred[\"classes\"]\n",
    "\n",
    "    pred_boxes = decode_regression_to_boxes(box_pred)\n",
    "\n",
    "    pred_scores = cls_pred\n",
    "\n",
    "    anchor_points, stride_tensor = get_anchors(x.shape[1:])\n",
    "\n",
    "    stride_tensor = tf.expand_dims(stride_tensor, axis=-1)\n",
    "\n",
    "    gt_labels = y[\"classes\"]\n",
    "\n",
    "    mask_gt = tf.reduce_all(y[\"boxes\"] > -1.0, axis=-1, keepdims=True)\n",
    "\n",
    "    gt_bboxes = bounding_box.convert_format(\n",
    "\n",
    "        y[\"boxes\"],\n",
    "\n",
    "        source=self.bounding_box_format,\n",
    "\n",
    "        target=\"xyxy\",\n",
    "\n",
    "        images=x,\n",
    "\n",
    "    )\n",
    "\n",
    "    pred_bboxes = dist2bbox(pred_boxes, anchor_points)\n",
    "\n",
    "    print(f\"shape pred_scores: {tf.shape(pred_scores)}\")\n",
    "\n",
    "    print(f\"shape pred_boxes: {tf.shape(pred_boxes)}\")\n",
    "\n",
    "    print(f\"shape pred_bboxes: {tf.shape(pred_bboxes)}\")\n",
    "\n",
    "    print(f\"shape anchor_points: {tf.shape(anchor_points)}\")\n",
    "\n",
    "    print(f\"shape stride_tensor: {tf.shape(stride_tensor)}\")\n",
    "\n",
    "    print(f\"shape gt_labels: {tf.shape(gt_labels)}\")\n",
    "\n",
    "    print(f\"shape gt_bboxes: {tf.shape(gt_bboxes)}\")\n",
    "\n",
    "    print(f\"gt_bboxes dtype: {gt_bboxes.dtype}\")\n",
    "\n",
    "    target_bboxes, target_scores, fg_mask = self.label_encoder(\n",
    "\n",
    "        pred_scores,\n",
    "\n",
    "        tf.cast(pred_bboxes * stride_tensor, gt_bboxes.dtype),\n",
    "\n",
    "        anchor_points * stride_tensor,\n",
    "\n",
    "        gt_labels,\n",
    "\n",
    "        gt_bboxes,\n",
    "\n",
    "        mask_gt,\n",
    "\n",
    "    )\n",
    "\n",
    "    target_bboxes /= stride_tensor\n",
    "\n",
    "    target_scores_sum = tf.maximum(tf.reduce_sum(target_scores), 1)\n",
    "\n",
    "    box_weight = tf.expand_dims(\n",
    "\n",
    "        tf.reduce_sum(target_scores, axis=-1) * fg_mask, axis=-1,\n",
    "\n",
    "    )\n",
    "\n",
    "    y_true = {\n",
    "\n",
    "        \"box\": target_bboxes * fg_mask[..., None],\n",
    "\n",
    "        \"class\": target_scores,\n",
    "\n",
    "    }\n",
    "\n",
    "    y_pred = {\n",
    "\n",
    "        \"box\": pred_bboxes * fg_mask[..., None],\n",
    "\n",
    "        \"class\": pred_scores,\n",
    "\n",
    "    }\n",
    "\n",
    "    sample_weights = {\n",
    "\n",
    "        \"box\": self.box_loss_weight * box_weight / target_scores_sum,\n",
    "\n",
    "        \"class\": self.classification_loss_weight / target_scores_sum,\n",
    "\n",
    "    }\n",
    "\n",
    "    return super(YOLOV8Detector, self).compute_loss(\n",
    "\n",
    "        x=x, y=y_true, y_pred=y_pred, sample_weight=sample_weights, **kwargs\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Bind the new compute_loss method\n",
    "\n",
    "YOLOV8Detector.compute_loss = compute_loss\n",
    "\n",
    "\n",
    "\n",
    "# Compile and fit the model\n",
    "\n",
    "yolo.compile(\n",
    "\n",
    "    optimizer=optimizer, classification_loss=\"binary_crossentropy\", box_loss=\"ciou\"\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "yolo.fit(\n",
    "\n",
    "    train_dataset,\n",
    "\n",
    "    validation_data=val_dataset,\n",
    "\n",
    "    epochs=EPOCH\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfbd73d-d200-46d6-9b91-e43de8a0892e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
