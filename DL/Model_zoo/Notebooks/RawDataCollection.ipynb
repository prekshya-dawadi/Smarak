{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76474c7a",
   "metadata": {},
   "source": [
    "# Extraction from video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85fa249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "303e11f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_folder = \"D:\\Projects\\DL\\MonumentDetection\\Dataset\\Raw\\BhaktapurDurbarsquare-20240325T142729Z-001\\BhaktapurDurbarSquare\"\n",
    "videos_list = []\n",
    "\n",
    "for file in os.listdir(videos_folder):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".mp4\"):\n",
    "        videos_list.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f410b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Total Extracted Frames: 38\n"
     ]
    }
   ],
   "source": [
    "for video_pa in videos_list:\n",
    "    video_path = os.path.join(videos_folder, video_pa)\n",
    "    try:\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        success = True\n",
    "        count = 1\n",
    "        image_id = 1\n",
    "            \n",
    "        while success:\n",
    "            success, frame = video.read()\n",
    "            if success:\n",
    "                if count % 5 == 0:\n",
    "                    name = str(image_id) + \".jpg\"\n",
    "                    cv2.imwrite(name, frame)\n",
    "                    image_id += 1\n",
    "                count += 1\n",
    "            else:\n",
    "                print(\"Not successful. Failed to read frame.\")\n",
    "    except cv2.error as e:\n",
    "        print(f\"Error occurred while capturing frames from video: {e}\")\n",
    "\n",
    "print(\"Total Extracted Frames:\", image_id - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6ad29b",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9700d2ff",
   "metadata": {},
   "source": [
    "### Download from saved urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc652e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_555 has been scraped\n",
      "image_556 has been scraped\n",
      "image_557 has been scraped\n",
      "image_558 has been scraped\n",
      "image_559 has been scraped\n",
      "image_560 has been scraped\n",
      "image_561 has been scraped\n",
      "image_562 has been scraped\n",
      "image_563 has been scraped\n",
      "image_564 has been scraped\n"
     ]
    }
   ],
   "source": [
    "# Download image objects\n",
    "counter = 555\n",
    "# iterates over each dictionary object in \"image_links\" list\n",
    "for image_object in image_links:\n",
    "    #Create a new .png image file\n",
    "    try:\n",
    "        # opens a new file in write mode from filename constructed from image title\n",
    "        filename = f\"image_{counter}\"\n",
    "        counter += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error occured while assigning filenames: {e}\")\n",
    "    \n",
    "    try:\n",
    "        with open(f\"./Dataset/ScrapedImages/{filename}.png\", \"wb\") as file:\n",
    "            # sends HTTP GET request to the image link specified in the current \"image_object\"\n",
    "            # uses httpx.get() to fetch image from its URL\n",
    "            image = httpx.get(image_object[\"link\"])\n",
    "            # Save the image binary data into the file\n",
    "            file.write(image.content)\n",
    "            print(f\"{filename} has been scraped\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occured while downloading the image: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a41ceea",
   "metadata": {},
   "source": [
    "### Without Drivers for Static Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306ef1eb",
   "metadata": {},
   "source": [
    "Extraction from id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b86490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc26a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find image links on the website\n",
    "\n",
    "\n",
    "# Initializes empty list called \"image_links\" where the scraped image links and titles will be stored\n",
    "image_links = []\n",
    "\n",
    "# Parent ids that identifies each image \n",
    "parent_div_ids = [\"Lun_Dhwākhā_(Golden_Gate)\", ]\n",
    "\n",
    "# Specifies the number of pages to scrape\n",
    "numberOfPages = 4\n",
    "#Scrape the first n pages\n",
    "for page in range(numberOfPages):\n",
    "    # Constructs a url for each page to be scraped\n",
    "    url = f\"https://web-scraping.dev/products?page={page}\"\n",
    "    # Sends an HTTP GET request to the URL constructed for the current page and stores the response\n",
    "    response = httpx.get(url) # httpx.get() sends an HTTP GET request to the URL\n",
    "    # the response it gives is a response object, stored in response variable.\n",
    "    # response can be further analysed to extract data, such as HTML content, headers, or any other \n",
    "    # information provided by the server. In this snippet, the HTML content of the webpage is extracted from the\n",
    "    # response in order to perform web scraping using beautiful soup.\n",
    "    \n",
    "    # Parses the HTML content of the webpage using beautiful soup\n",
    "    # response.txt extracts the textual content of the response. In this case, its likely\n",
    "    # the HTML content of the webpage that was requested. \n",
    "    # BeautifulSoup used for parsing HTML and XML documents. Creates a parse tree\n",
    "    # from the HTML source code of the webpage, which can then be used to extract data\n",
    "    # \"html.parser\" specifies the parser to be used by Beautiful Soup. In this case, indicates that\n",
    "    # Beautiful Soup should use the built-in HTML parser provided by Python's standard library\n",
    "    # soup: This variable holds the beautiful soup object that represents the parsed HTML structure \n",
    "    # of the webpage. Allows us to navigate and seach through the HTML elements of the webpage in a more \n",
    "    # convenient and pythonic way.\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # Selects all HTML elements with the class \"row\" and \"product\" within \"<div>\" tag. \n",
    "    # Assumes that each product on the webpage is contained within a \"<div>\" element with these classes\n",
    "    \n",
    "    # Iterate over each id\n",
    "    for parent_div_id in parent_div_ids:\n",
    "        for image_box in soup.select(f\"div[id^='{parent_div_id}']\"):\n",
    "            # For each selected product, it extracts the image link(\"src\" attribute of the \"<img>\" tag)\n",
    "            # and the title(text content of the \"<h3\" tag) and stores them in a dictionary named \"result\"\n",
    "            result = {\n",
    "                \"link\": image_box.select_one(\"img\").attrs[\"src\"],\n",
    "                \"title\": image_box.select_one(\"h3\").text,\n",
    "            }\n",
    "            #Append each image and title to the result array\n",
    "            image_links.append(result)\n",
    "\n",
    "print(image_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671259e2",
   "metadata": {},
   "source": [
    "Extraction from picture element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bac9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find image links on the website\n",
    "\n",
    "\n",
    "# Initializes empty list called \"image_links\" where the scraped image links and titles will be stored\n",
    "image_links = []\n",
    "urls = [\"https://www.bhaktapur.com/discover/nyatapola-temple/\", \"https://www.bhaktapur.com/discover/golden-gate/\"]\n",
    "\n",
    "for url in urls:\n",
    "    \n",
    "    response = httpx.get(url) \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # selects all <picture> elements on the page and iterates through\n",
    "    for picture_tag in soup.select(\"picture\"):\n",
    "        # Looks for a <source> tag inside it, if found, extracts the \"srcset\" attribute and\n",
    "        # if not found, looks for an <img> tag and extracts the \"src\" attribute else move on\n",
    "    \n",
    "        # Find the source element within each picture tag\n",
    "        source_tag = picture_tag.select_one(\"source\")\n",
    "        if source_tag:\n",
    "            # Extract the source url\n",
    "            source_url = source_tag.attrs.get(\"srcset\", \"\")\n",
    "        else:\n",
    "            # If source tag is not found, try to get the img tag\n",
    "            img_tag = picture_tag.select_one(\"img\")\n",
    "            if img_tag:\n",
    "                # Extract the image URL\n",
    "                source_url = img_tag.attrs.get(\"src\", \"\")\n",
    "            else:\n",
    "                # Skip if neither source not img tag is found\n",
    "                continue\n",
    "        # Extract title if available from h3(?)\n",
    "        title = picture_tag.find_previous(\"h1\").text.strip() if picture_tag.find_previous(\"h1\") else \"\"\n",
    "    \n",
    "        # Append the image link and title to the result array\n",
    "        image_links.append({\"link\": source_url, \"title\": title})\n",
    "\n",
    "print(image_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444271de",
   "metadata": {},
   "source": [
    "All the images from the site provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be70704e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 image links on the webpage.\n"
     ]
    }
   ],
   "source": [
    "image_links = []\n",
    "url = \"https://www.alamy.com/stock-photo/nyatapola.html?sortBy=relevant\"\n",
    "response = httpx.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "img_tags = soup.find_all(\"img\")\n",
    "\n",
    "for img_tag in img_tags:\n",
    "    src = img_tag.get(\"src\")\n",
    "    if src:\n",
    "        image_links.append({\"link\":src, \"title\": \"\"})\n",
    "print(f\"Found {len(image_links)} image links on the webpage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc5844d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
