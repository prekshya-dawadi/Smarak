{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b263395",
   "metadata": {},
   "source": [
    "# Extraction from video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85fa249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "303e11f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_folder = \"D:\\Projects\\DL\\MonumentDetection\\Dataset\\Raw\\BhaktapurDurbarsquare-20240325T142729Z-001\\BhaktapurDurbarSquare\"\n",
    "videos_list = []\n",
    "\n",
    "for file in os.listdir(videos_folder):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".mp4\"):\n",
    "        videos_list.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f410b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Not successful. Failed to read frame.\n",
      "Total Extracted Frames: 38\n"
     ]
    }
   ],
   "source": [
    "for video_pa in videos_list:\n",
    "    video_path = os.path.join(videos_folder, video_pa)\n",
    "    try:\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "        success = True\n",
    "        count = 1\n",
    "        image_id = 1\n",
    "            \n",
    "        while success:\n",
    "            success, frame = video.read()\n",
    "            if success:\n",
    "                if count % 5 == 0:\n",
    "                    name = str(image_id) + \".jpg\"\n",
    "                    cv2.imwrite(name, frame)\n",
    "                    image_id += 1\n",
    "                count += 1\n",
    "            else:\n",
    "                print(\"Not successful. Failed to read frame.\")\n",
    "    except cv2.error as e:\n",
    "        print(f\"Error occurred while capturing frames from video: {e}\")\n",
    "\n",
    "print(\"Total Extracted Frames:\", image_id - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8eb661",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64a6b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fee29ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find image links on the website\n",
    "\n",
    "\n",
    "# Initializes empty list called \"image_links\" where the scraped image links and titles will be stored\n",
    "image_links = []\n",
    "# Specifies the number of pages to scrape\n",
    "numberOfPages = 4\n",
    "#Scrape the first n pages\n",
    "for page in range(numberOfPages):\n",
    "    # Constructs a url for each page to be scraped\n",
    "    url = f\"https://web-scraping.dev/products?page={page}\"\n",
    "    # Sends an HTTP GET request to the URL constructed for the current page and stores the response\n",
    "    response = httpx.get(url) # httpx.get() sends an HTTP GET request to the URL\n",
    "    # the response it gives is a response object, stored in response variable.\n",
    "    # response can be further analysed to extract data, such as HTML content, headers, or any other \n",
    "    # information provided by the server. In this snippet, the HTML content of the webpage is extracted from the\n",
    "    # response in order to perform web scraping using beautiful soup.\n",
    "    \n",
    "    # Parses the HTML content of the webpage using beautiful soup\n",
    "    # response.txt extracts the textual content of the response. In this case, its likely\n",
    "    # the HTML content of the webpage that was requested. \n",
    "    # BeautifulSoup used for parsing HTML and XML documents. Creates a parse tree\n",
    "    # from the HTML source code of the webpage, which can then be used to extract data\n",
    "    # \"html.parser\" specifies the parser to be used by Beautiful Soup. In this case, indicates that\n",
    "    # Beautiful Soup should use the built-in HTML parser provided by Python's standard library\n",
    "    # soup: This variable holds the beautiful soup object that represents the parsed HTML structure \n",
    "    # of the webpage. Allows us to navigate and seach through the HTML elements of the webpage in a more \n",
    "    # convenient and pythonic way.\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # Selects all HTML elements with the class \"row\" and \"product\" within \"<div>\" tag. \n",
    "    # Assumes that each product on the webpage is contained within a \"<div>\" element with these classes\n",
    "    for image_box in soup.select(\"div.row.product\"):\n",
    "        # For each selected product, it extracts the image link(\"src\" attribute of the \"<img>\" tag)\n",
    "        # and the title(text content of the \"<h3\" tag) and stores them in a dictionary named \"result\"\n",
    "        result = {\n",
    "            \"link\": image_box.select_one(\"img\").attrs[\"src\"],\n",
    "            \"title\": image_box.select_one(\"h3\").text,\n",
    "        }\n",
    "        #Append each image and title to the result array\n",
    "        image_links.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c1b415",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './images/Box of Chocolate Candy.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Download image objects\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_object \u001b[38;5;129;01min\u001b[39;00m image_links:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m#Create a new .png image file\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./images/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_object[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      5\u001b[0m         image \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mget(image_object[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;66;03m# Save the image binary data into the file\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './images/Box of Chocolate Candy.png'"
     ]
    }
   ],
   "source": [
    "# Download image objects\n",
    "\n",
    "# iterates over each dictionary object in \"image_links\" list\n",
    "for image_object in image_links:\n",
    "    #Create a new .png image file\n",
    "    \n",
    "    # opens a new file in write mode from filename constructed from image title\n",
    "    with open(f\"./Dataset/ScrapedIm{image_object['title']}.png\", \"wb\") as file:\n",
    "        # sends HTTP GET request to the image link specified in the current \"image_object\"\n",
    "        # uses httpx.get() to fetch image from its URL\n",
    "        image = httpx.get(image_object[\"link\"])\n",
    "        # Save the image binary data into the file\n",
    "        file.write(image.content)\n",
    "        print(f\"Image {image_object['title']} has been scraped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e798190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
