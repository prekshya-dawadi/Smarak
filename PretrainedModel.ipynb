{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2918d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f697435",
   "metadata": {},
   "source": [
    "# Structure for detection task\n",
    "\n",
    "1. Dataloaders:\n",
    "    - Input images for predicton of detection task: \n",
    "         * preprocess_input(): process original images to image data\n",
    "         * decoder(): convert image data to tensor\n",
    "         * parser(): breaks the input into parts\n",
    "    - Load pre-trained weights into YOLOv3 model:\n",
    "        * (utils) download_weights()\n",
    "        * load_weights_to_model()\n",
    " 2. Modeling:\n",
    "     - nn_block() or layers:\n",
    "     - yolo_model():\n",
    " 3. Ops:\n",
    "     - decode_netout(): decode the prediction of detection model into boxes \n",
    "     - correct_boxes():\n",
    "     - nms() and bbox_iou():\n",
    "     - draw_boxes():\n",
    " 4. Configs:\n",
    "     - Image Input:\n",
    "         * image_path:\n",
    "         * num_classes:\n",
    "         * input_size: for modeling\n",
    "     - Pretrained weights:\n",
    "         * pb_file: pretrain weights\n",
    " \n",
    " 5. Tasks:\n",
    "     - build_input():\n",
    "     - build_model():\n",
    "     - build_loss():\n",
    "     - metrics:\n",
    " \n",
    " 6. Common/Import Registry:\n",
    "     - Include the components above\n",
    " \n",
    " 7. Train:\n",
    "     - train.py\n",
    " \n",
    " 8. Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2b18ad6-890c-44df-af5a-096de09bc72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dc84fe",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857a2d5",
   "metadata": {},
   "source": [
    "Reads, Decodes and Parses the input data. Contains a decoder and Parser.\n",
    "\n",
    "Decoder decodes a TF record and returns a dictionary of decoded tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e2f7d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_models\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfm\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mofficial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataloaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_example_decoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfExampleDecoder\n\u001b[0;32m      4\u001b[0m pp \u001b[38;5;241m=\u001b[39m pprint\u001b[38;5;241m.\u001b[39mPrettyPrinter(indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;66;03m# Set Pretty Print Indentation\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_models'"
     ]
    }
   ],
   "source": [
    "import tensorflow_models as tfm\n",
    "from official.vision.dataloaders.tf_example_decoder import TfExampleDecoder\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4) # Set Pretty Print Indentation\n",
    "print(tf.__version__) # Check the version of tensorflow used\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "432f6c4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mYOLODecoder\u001b[39;00m(decoder\u001b[38;5;241m.\u001b[39mDecoder): \u001b[38;5;66;03m# inherits from decoder.Decoder provided by Tensorflow Model Garden\u001b[39;00m\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"Initializes the decoder.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m        Defines mapping between the field name and value from an input. \u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m        E.g. We define two fields for image bytes and labels.\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decoder' is not defined"
     ]
    }
   ],
   "source": [
    "class YOLODecoder(decoder.Decoder): # inherits from decoder.Decoder provided by Tensorflow Model Garden\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the decoder.\n",
    "        Defines mapping between the field name and value from an input. \n",
    "        E.g. We define two fields for image bytes and labels.\"\"\"\n",
    "        \n",
    "        # defines a dictionary that contains mapping between feature keys and \n",
    "        # their corresponding feature types.\n",
    "        self._keys_to_features = {\n",
    "            # FixedLenFeature is used when the feature has a fixed length. It expexts\n",
    "            # a single value or a tensor of a specified shape and data type.\n",
    "            # 'image/encoded' is a fixed-length feature representing the encoded image data. \n",
    "            # The shape is specified as '()' since its a single string (the image data), and the data type is \n",
    "            # 'tf.string'.\n",
    "            # VarLenFeature is used when the feature has a variable length. It expects a\n",
    "            # sparse tensor representing a list of values of the specified data type. \n",
    "            # 'image/object/bbox/xmin', 'image/object/bbox/ymin', 'image/object/bbox/xmax', \n",
    "            # 'image/object/bbox/ymax', and 'image/object/class/label' are variable-length features.\n",
    "            'image/encoded': tf.io.FixedLenFeature((), tf.string, default_value=''),\n",
    "            #'image/class/label': tf.io.FixedLenFeature((), tf.int64, default_value = -1)\n",
    "            # Since the number of bounding boxes and labels can vary from image to image, \n",
    "            # it's more efficient to use variable-length features. \n",
    "            'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n",
    "            'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n",
    "            'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n",
    "            'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n",
    "            'image/object/class/label': tf.io.VarLenFeature(tf.float32),\n",
    "        }\n",
    "        \n",
    "    def decode(self, features): # Features in a dictionary containing decoded tensors\n",
    "        \n",
    "        # decode image\n",
    "        \n",
    "        # decodes the encoded image data ('image/encoded') using tf.image.decode_jpeg. decodes jpeg-encoded images into uint8 tensors. channels = 3 indicates RGB.\n",
    "        image = tf.image.decode_jpeg(features['image/encoded'], channels = 3)\n",
    "        \n",
    "        # Decode the bounding boxes\n",
    "        \n",
    "        # decodes sparse tensors to dense tensors\n",
    "        xmin = tf.sparse.to_dense(features['image/object/bbox/xmin'])\n",
    "        ymin = tf.sparse.to_dense(features['image/object/bbox/ymin'])        \n",
    "        xmax = tf.sparse.to_dense(features['image/object/bbox/xmax'])        \n",
    "        ymax = tf.sparse.to_dense(features['image/object/bbox/ymax'])        \n",
    "        labels = tf.sparse.to_dense(features['image/object/class/label'])        \n",
    "        \n",
    "        \n",
    "        # Combine bounding box coordinates\n",
    "        \n",
    "        # stacks the decoded bounding box coordinates ('xmin', 'ymin', 'xmax', 'ymax') along the last axis\n",
    "        # ('axis = -1') to form a tensor of shape '(num_boxes, 4)'.\n",
    "        # Each row of this tensor represents a bounding box with coordinates '[xmin, ymin, xmax, ymax]'\n",
    "        boxes = tf.stack([xmin, ymin, xmax, ymax], axis = -1)\n",
    "        \n",
    "        return image, boxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152f5e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
